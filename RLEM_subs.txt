um and so

this is the the ideal platform to

exchange um this year we couldn't meet

in person

um that means we can't actually talk at

the whiteboard which is what we would

have liked to do

on the other hand um we have already 71

people in this channel

there were about 250 signups and so

that's an

a big number that we have not

anticipated this way and that's

the the positive things that came out of

um having to do this virtually as as you

can imagine

like i said it is about you before i get

to that

the organization you see we have three

sessions and we'll get to those

in two seconds the important part is

that the talks in those sessions

are scheduled so that we should be done

with all the talks about 15 minutes

ahead of time

so there is ample time afterwards for a

discussion between the panelists

uh you know with the audience keep your

keep your um keep keep your big picture

questions to that end ask maybe

technical questions right after the talk

and then we can have a bigger bigger

conversation a little bit off from

what typical conference uh schedule

would be

so that's that's one thing the other

thing is look at the audience

um and just to remind you this is a very

very diverse audience from many

perspectives

one the backgrounds the backgrounds is

going from you know graduate students

all the way up to

faculty and then also from you know

university backgrounds to

private private companies um all the way

to ctos managers project managers

and also a lot of colleagues at uh

research labs lbnl uh

pnnl rnl and so on um that are you know

leading

uh leading the way and doing a lot of

projects in this field and so what i

want to remind you is when you're

discussing it

that you know the audience is very

diverse and so keep that in mind

when you're addressing questions and

when you're answering those questions

one questions that we ask you ahead of

time is what you expect out of this

workshop

and you know i was trying to make a

really

interesting way to visualize that but

the word

cloud only showed reinforcement and

learning and so i removed that and

that's kind of just show you a few of

these outcomes in the video

and you can read them it a lot of it is

you know learn more

but a lot of it is also very specific

you saw their reward shaping going up

there

some of it was how do we apply this um

in my specific field some of it is you

know

specifically in building control and so

a lot of questions a lot of good

questions and so i i think the seats

have been laid

for a very good workshop

and so with that i would like to thank

you for coming i would like you to thank

you for

attending thank you for submitting your

papers

and i'll pass it on to june we'll walk

you through the program

thank you

thank you dr nikki so together with the

mario

and bing ching we tpc chairs uh

try to review i mean try to review

papers and make a good program for you

for the ireland workshop so basically

two reviewers

thoroughly reviewed each paper and we

selected 12

papers and then grouped them into three

sessions

so the right side i just like

make a word cloud map based on the

abstract submitted from that total

papers

as you can see over there on the top

side electricity demand

as a application side of this rlam and

then the bottom side

of course you can see rl as a big chunk

and then there is a hvac and some

technical terms like

more state and reward action space

set an action space those keywords are

also

popped up on the top and then another

middle is a problem

like solving this very dynamic problem

and then there's a software

and water and other some technical

keywords in the bottom

and we would like to thank all the

technical pro

program committee members for your

spending time on reviewing those papers

those 13 people really great next please

yeah so the three sessions are first is

a demanding response

as i said earlier it's more on

application side of

how to code coordinating the the

electrical grade

and next please

and the second theme was clash of

algorithms which is more focusing on

some theoretical part of algorithmic

side of the rl

this is our limb field next please

the third one is a keeping it real and

when we apply rl into energy management

system

what kind of uh some challenges related

to

more like build implementation or

software implementation

and those papers are addressing those

things

and we would like to also thank all the

contributors

authors and co-authors of this paper to

this great program

thanks please

so as a reminder for presenters of each

paper

please change your zoom name as

a session number and your name so that

our

technology chair can make you co-host so

you

can share your screen easily so please

do that if you're presenter of those

12 nice papers great papers

next please alright so it's

now time for group photo time so please

turn on your camera and make a smile

so that we can capture all your

beautiful faces

so

kingsley is doing the pictures kingsley

you have to tell um

oh yeah so when you're ready

not a requirement that you open your

camera if you're not ready for it

because it may be too early or too late

for some people it's okay but

if you can alright smile please

thank you i think i have everyone

okay awesome thank you everyone

appreciate it

and we'll share it later in this live

channel for everyone

um then let me move back here

okay then mario the floor is yours

all right so before i introduce the

speaker can i ask kinsley or someone to

bring him up as a panelist and ask that

he can

share his screen and uh also

enable his video

i have my video on i it was like as it

as it happened

right before the picture like my camera

stopped working and so i was not in the

picture sadly but

that's fine that's fine thank you

well so uh i'm very very can't share my

screen right now

we have your pictures rico no problem

i have not been co-host no oh no

oh now i am okay okay all right yes

i can share screen now great

um it is my pleasure my real pleasure to

introduce

zico coulter um zico

is a faculty member in the computer

science department at carnegie mellon

university

he also is a chief scientist for

ai research at the newly created bosch

center for ai that's

has many locations but there's one in

pittsburgh which

he's leaving and he does a lot of

interesting work

i think there's certainly very

little time for all the questions that

i'm guessing will come up

after his talk um i know zico from

way way back um we we started working on

non-interest reload monitoring together

um or at least at the same time not

necessarily working collaborating

together

and i personally appreciate all of the

work that he's putting out

um i'm sure that you guys will i don't

think that there's

any more to say other than waiting for

everything that you

will be talking to us uh about zico so

floor is yours take it away you like

great well thank you very much mario uh

thanks everyone for having me here

and uh i'm gonna talk today about

a very recent paper we have this paper

uh

we put on archive it went up an archive

last night um

it's been around for a while longer than

that but that was the deadline i told

them they had to get it

before the conference before the

presentation today this is also the

first time i

am talking about it so that you know

forgive any bugs and such in

the in the slides as you know when these

things happen uh

there's there's some kinks sometimes i'm

still testing out how we're gonna

exactly present all of this but um

do feel free i should add to ask

questions during this time i would

rather

um and in the chat or or just speak uh

or raise your hand in the participants

probably the chat's the easiest actually

i'm pretty good from classes at looking

at the chat while i'm presenting

um so ask questions that go um it is

more important that we have a fun

discussion

than uh that i get through all my slides

that's the the more important thing

and that i convey the basic ideas of

what we do uh

and so if there are if you're confused

as chances are a lot of people are also

confused so so

please please do uh that being said

there is some there is some sort of

dense math at times here uh some of us

kind of ingest almost because i won't

like to poke fun at how complex robust

control is

um but there is some math that is sort

of pertinent to our actual approach

so i do want to um sort of uh if

if there is some confusions there i'm

happy to go through it but i will try to

pop out sort of regularly and also

address the high level

kind of concept concepts of what we're

actually we're actually talking about

here

all right so the the actually the most

important thing uh to mention is that

this is joint work with

um two of my students uh priyadonti and

mel roderick so this is this is

you can see my mouse right uh it's not

it's not ideal but it's

going to have to act as a as a cursor

for now or as a little pointer for now

and mel roderick um as well as a

collaboration with

uh uh

formerly at upenn starting a fat

position at johns hopkins shortly

all right so um i am going to start out

at a pretty

uh with a pretty high pretty high level

uh and ask and and talk about deep

reinforcement learning right this is a

conference on reinforcement learning

um and so you know i want to talk about

what deep reinforcement learning has

done because it's done some pretty

amazing things recently

this is why to a certain extent we're

all here right

so so you know some of the it's going to

be a very biased uh

sample of things but you know deep

reinforcement learning has done some

amazing things

in 2015 it's a famous nature paper

actually the result was in 2013 but 2015

it was famous nature paper on learning

to play atari um

out of deep mind and so they learned to

play in all his atari games from using

deep reinforce a deep q-learning system

um later on that same team

i keep losing the ability to go forward

here yeah okay that same team uh

famously

defeated um well the same company right

different team there

famously um defeated one of the sort of

masters

of the game of go with their alphago

system

um and so this was an advance that i

think was

seen at the time as being maybe 10 years

away you know computer go had been

progressing sort of slowly and steadily

up until that point

and all of a sudden in uh in 2016 there

was this major breakthrough

where where a system also using deep

reinforcement learning

beat um not he wasn't quite the world

leader right then i think it's like

number four in the world

uh but be you know one of the best

players in the world

and since then has improved even more to

sort of be really unbeatable by humans

um and then maybe you know you could

always claim that oh well go

is um go is easy uh because

it you know it's full fully observed you

know the full state etc uh but there's

but there's been other

very cool things as well so open ai in

2019 released

a deep rl system that that played um

the game of dota 2 i don't know that

even stands for it's a it's like a

real-time strategy game like

like warcraft um

and by again using basically you know

pretty straightforward conceptually

though of course there's a lot of

uh complexity in the system but

conceptually kind of straightforward

deep reinforcement outcomes they were

able to

kind of get state of the art on a you

know long time horizon

uh imperfect information

you know strategic multiplayer game

which is sort of one of the hardest

classes of games

to a large extent and so the question

sort of is you know

if we have all of this stuff here um

why isn't deep rl being used kind of

everywhere right these are really really

hard problems and so why

isn't dprl kind of funny application

everywhere we have a lot of control

tasks

and buildings and a lot of other control

systems

uh why isn't deep rl being deployed

there

um and so you know if it's so great here

um there's a lot of tests we like to

solve better maybe they're already kind

of solved but we'd like to solve them

better

um so you know what's what's stopping us

from doing

my uh hang on here

my cursor is not advancing

what's stopping them was something else

from using you know deep rl for

i don't know control service control for

airplanes right

we have pretty good stabilization

systems for airplanes already they fly

pretty well right they don't they don't

fall in this guy but you know

they could be improved certainly we can

make them more efficient things like

this right

so why isn't it being used there and i

think

the answer is actually sort of pretty

clear also right so when it comes to

sort of deep reinforcement learning and

reinforcement as a whole right it really

is operating on a different paradigm

than than sort of we're used to in a lot

of control tasks

and so a lot of control tasks you know

what we associate with deep rl are

things like

performance but also things like

exploration right so

you know we don't know the system we

have to explore the system to figure out

you know what's going what what does

what

and what goes wrong and we also

associate it with very expressive

policies so

control policies that can do you know

almost anything with very deep networks

and then things like this

but for which it's extremely hard to

provide any guarantees

and we can contrast this with a system

that does work

in kind of modern you know aviation but

well beyond that which is this sort of

broad domain of robust control

so these are not new problems right

we've been controlling

uh airplanes for more than 100 years now

and at least in sort of modern uh

implementation of things we actually

have very well established methods for

controlling them right we have you know

methods from from uh pi control and

robust control these kinds of things

that really

work pretty well for those tasks

and what's associated with these things

besides of course the the fact that

control theories also

create much worse looking graphics um

than than deep mind

um that's also a big difference this is

from i think phil koopman's course

uh of course notes uh on robust control

um

but the things associated that the goal

of robust control to a certain degree

are are guarantees right um

and when you do control an airplane you

know you

you really want to make sure it doesn't

fall out of the sky accidentally and

yet to explore that you know 100 times

before it starts to work right

you really want to have guarantees about

stability about about

robustness and typically it uses

to achieve this it uses fairly simple um

policies like linear policies so

sometimes these are very different

worlds right and they apply to very

different things um deep rl applies

racially the sort of things that we

don't really know how to solve very well

but

we want to kind of turn over complete

control to an algorithm

typically in simulation let it run muck

a whole lot

and then figure things out whereas

robust control plus the domains where

you know we want to carefully

characterize the precise level of

uncertainty

of the real system solve that system

with the

with uncertainty to a degree where we

can provide a guarantee of stability

there

and then deploy that on the real system

but we are with much less chance

and still tested of course the real

system you know we don't do any testing

uh but it can be much less testing

because we sort of have to just you know

validate that in fact system does

obey our our um assumptions about its

its uncertainty and if we have that then

we kind of know that it works

and that's that's sort of the robust

control paradigm um

so this talk is about this is about kind

of um the middle ground between these

two

these two worlds here uh so can we start

to kind of

explore the very wide space

that lies between these two extremes

and in particular i'm going to talk

about how we can

embed robust control policies within

deep networks so that's going to have

three parts i'm not even really

bothering with sections here i'll just

sort of call out where i'm where i'm

changing those

but first to do this i'm first going to

talk about robust control

um then i'm going to talk about how we

and sort of what the guarantees of bus

control are because i'm actually working

with a very specific

type of robust control here or we are

and

therefore i want to sort of define what

i mean by that

then i'll talk about how we can

incorporate those

same guarantees you have from robust

control

into neural network policies

and that's something that's almost

surprising right it seems surprising

that we can actually give the same level

of guarantees

as we have for a robust control system

for a neural network policy

but it turns out we actually can do that

and once you do that

you can actually then train these things

using standard deep reinforcement

learning

while still having the guarantees

of robust control so this is the outline

of the talk today

and i'll follow with some some brief

experimental results

but then probably more importantly

highlight some open questions at the end

okay so let's jump right in um to

robust control

so robust control is a field that's

broadly concerned with methods that are

certifiably robust to worst

disturbances within some specified

disturbance

region and that's what robust control

means it means that aor the system will

still perform well

even when it's being disturbed kind of

in the worst case

by some adversarial perturbation as long

as that perturbation is bounded in a

specific way

so i want to give one example of these

things i think it helps a lot to sort of

be concrete with examples here but i am

but it is going to be a little bit

mathematical now

so one common example of these robust

controls specifications or the kind of

guarantees that we can get out of them

uh involves something called a norm

bounded linear differential inclusion

so this is it's a very simple system

actually um but it can capture very

complex potentially

complex dynamics so the idea here is

that we have some states so some of the

state of our system is

i'm going to know as xt here that's the

state of our system it's it's the you

know

the the angle of attack of the airplane

the speed you're going

or it's the whole game board if you're

playing go and things like that right

it's the whole state of the system right

now

and we're gonna talk about continuous

time systems here i think that's

actually sort of more common in

traditional control

so we're going to talk about continuous

time systems meaning i'm going to

describe

the state evolution in terms of the time

derivative of that state

so i'm providing a model for how this

data falls as essentially

a differential differential equation but

because it's a cassette it's

called differential inclusion all right

so

so so the the way we model uh a

particular linear differential inclusion

is to say that this the time derivative

of the state

is equal to um uh some linear function

of the state vector

plus some linear function of the control

vector the control could be you know how

much you control the ailerons in the in

the plane et cetera

plus and this if it was just this

portion here this would just be a linear

dynamical system

of which we there are many techniques to

analyze but

plus we add this extra term here which

characterizes a potential disturbance

we apply the system and this disservice

can be really anything it's not it's not

just random noise it can be really

anything you have it can be any

arbitrary disturbance including sort of

adversarial disturbances that try to

penalize the system as much as possible

but the key point here is in most robust

control specifications and i should

add there are a lot of these right there

are a ton of these like h infinity

control there's a slightly different

thing all these things do a little bit

different

but the thing that you do in robust

control

is that you bound the magnitude

of that disturbance in some manner okay

so so you one common way of bounding it

for example would be to say that the

the magnet's disturbance is bounded by

some function of

you know both the state vector and the

control vector okay that would be sort

of a standard way of doing that

again the details are not super

important here and there are also some

additional assumptions like the fact

that the zero state here is sort of our

actually ours our sort of our code for

our set point

so it's not me that doesn't mean we have

you know zero velocity it means that

zero state is just sort of the nominal

uh state of the system

um this is sort of modeling like

divergence from the nominal state

and um the the big idea here is that in

some sense what we're assuming

is that we're assuming that this

disturbance is bounded

as some function of the state deviate

state and control deviation

which is kind of reasonable right

because if you're right exactly at like

the equilibrium point that's sort of a

stable point to do anything

but as you deviate from that you know

non-linearities kick in

noise in your controls kicks in so all

these things happen that kind of make

the system

deviate from this nice linear system

that you would have normally and so

this is this this is this formalism that

captures that

right and the goal of robust control

is to find a control law which is

basically a function mapping the current

state

to the controller you should apply that

will stabilize this system

no matter what the disturbance is as

long as it's bounded this way

and this is what i mean this is an

oversimplification of course but you

know to it to a first approximation

this is what robust control is about of

course many different formulations

many different extensions you know

partially observable systems all this

kind of stuff but this is what remote

control is about

any questions so far i haven't seen it

yet but please do chime in if you have

any if you have any questions

i'll bring the chat window back up so i

can make sure i can see it

all right so so let me then move on um

all right so how do we do this how do we

construct a control law

that is actually uh can stabilize this

system here

well here's how you do it

[Music]

given this whole thing here

what you do is you solve this

semi-definite program here

so this is something different program

in some variables s y

and mu and then you solve this thing to

be positive definite

and then you choose k to be k is our

control law so k is the thing that map

states to

to controls um and then you choose k in

this manner

that's right so so obviously

um i'm i'm putting this slide up this

was this is kind of the this so hey this

is actually how you do it to be clear uh

this is this is this is how you do it

you solve this semi-definite program

um but the the funny i'm more putting

this up as kind of as

just a little a little bit of humor here

because if you read robust control

papers they're like full of these things

they're like

full of these like they're called linear

mixed inequalities

or lmi's they're full of these things um

and they basically all describe

conditions under which you can guarantee

stability of a system

under slightly different variants of

this kind of thing

um so i do want to highlight a little

bit what's going on in this i'm not

going to derive this this thing because

it actually i mean it's it's

it's quite involved to be honest um it

it uses this thing called the s

procedure and lots of sort of uh various

semi-definite programming techniques

but but really is actually what's what's

going on here is quite simple

um believe it or not so what's going on

here

is that all these methods and every time

you see one of those big lmi's

um uh those big semi-definite

expressions

all that they're trying to do in all

these things is construct what's called

a lyapunov function

so they're trying to construct a

positive definite function that just

means this function is positive

everywhere

except zero at zero so it's causing a

positive definite function

that is decreasing along trajectories of

the system

okay so what that means is this there's

this function v um and by the way the v

that you get on that previous one is one

of the variables

uh was s that you're solving for and v

as just the quadratic form

in s so

it's solving for lyapunov function and

this function you know by definition

here this function's always going to be

positive right so for any state

um this this this s here is a uh is a

positive definite matrix so it's

for so this function here is positive no

matter what

um and the other thing is happening here

is that

for any possible disturbance

this function will be decreasing so what

i think about that is that this function

is like

like like a sort of like a bowl that

measures some notion of energy of the

system

and this controller is always decreasing

this function

in other words it has to always be

pushing the system to the zero state

so if it's decreasing this function then

it's always going to be you know pushing

the function to towards or pushing the

state towards zero

and that's and that's what all these big

expressions are for

they are ultimately for ensuring that

there exists a valid diaphadon function

for the state

so really when it comes down to it what

robust control really does

is it says given some uncertainty

specification

like this one i gave a few slides ago

given that we describe a system like

this can we

find a lyapinov function

a function that sort of measures some

notion of energy of the system

such that this function we can find a

control

that always makes this function decrease

and this function's always positive and

so therefore it always you know goes to

goes

nicely drives us 6 to zero

but you don't have to worry in too much

about sort of sodium fluorides are the

form of this function

but this notion of stability

as dictated by the optimal function

actually is an important one and this is

the key thing that we're going to use

to ensure that deep reinforcement

learning policies

can also be guaranteed to be stable in

the same way

that's that's the core idea that we're

going to exploit here

so before i do that i want to make one i

sort of mentioned this a little bit

before but i want to make one sort of

important

note here um in that i'm writing our

system

as as in sort of in this form so i'm

running our system as you know

x dot is in this thing here uh

and i'm also assuming that you know uh a

b g uh as well as the other elements by

the way so so it's a

few points so you know uh going back to

here again

uh you know the things that characterize

this system right where the matrices a

b g as well as things that characterize

the bound which are the d

the c at d matrices right so these are

all matrices

because these are all vectors of the

state and everything like that um

and we're assuming that they're all

known so so so if if you look at this

uh lmi one more time i promise this last

time i'll have it up

um but look at this one more time it

actually the the the

there are terms that involve a b c d and

g

in there so you have to know those

things in order to solve

this lmi if you don't know them this

doesn't mean anything

so we assuming that we were assuming

that we know these here

um and so

the point that i want to make though is

that

assuming we know these matrices

does not imply that we actually know the

true underlying system

right so the whole point about robust

control is that we assume that true iron

system is not

actually known that it's some unknown

some unknown system and all that we

really know here are

bounds on that system right so in

particular

this disturbance term that i mentioned

before this w term

it can represent things like well

non-linearities

even if we know those because i still

represent those because the system is

linear here other than the disturbance

um but it can also model just unmodeled

dynamics too right so we can kind of

whatever sort of things we don't

understand at the system we can kind of

push

into that w term and as long as

we're able to bound it then we can still

provide guarantees as to the performance

of the system and this is really the

power of rust control is that even

without knowing

the true system you can still get

guarantees about its performance as long

as you're able to provide balance for it

now you can't do this for every system

of course right if the balance grow too

big

there will not be a solution to that

linear elegance the oxidation problem

will be

unsolvable it will not have a solution

but for

certain classes of systems you can in

fact provide a guaranteed

stable control law and the lyapunov

function that that will always work here

and that's the basic idea of robust

control

so remote control i mean at this point

it sort of seems great right like

you know um i guess there's sort of this

pain point of having actually come up

with these matrices but i don't know

we we know how to linearize systems um

maybe it doesn't really work for atari

but that's sort of okay

it does really work really well for you

know airplane control and stuff like

that

maybe we should just stick with it but

the question here and i think really the

motivating principle from from deeper

all of this worth thinking about

is can we do better than simple robust

control

because importantly robust control still

is talking about linear control laws

right we still have the control is still

a linear function

of the state that's not very expressive

you know we know

that non-linear controllers can do much

better

at actually solving these problems than

linear controllers

um and

i think every time someone enters the

room zoom takes back over control of my

screen

uh because i'm uh because i'm a

presenter now so

um and so you know is this a job for

deep rl

right maybe if we want to have more

expressive control laws

uh we should use dprl and we should be

using these things to actually solve for

you know the plain ailerons controls

module of the fact that again we

probably don't want to crash a bunch of

a bunch of planes before we actually

before we actually can solve this

so that's the setting so far so that's

actually all you know about robust

control hopefully not at least too many

people here

again high level points are there exist

these very powerful methods that create

control laws but they are very simple

control they guarantee stability

but they are very simple control laws

any questions

based upon that

no questions still okay everyone's like

an expert in this stuff already

i'm just rehashing like the basics here

move on to the more complex uh

formalisms here okay got it

all right

so this is a job for deep rl

so the deep rl paradigm is a little

different

of course than what we've talked about

so far so in traditional deeper

enforcement learning you kind of assume

the system is fully unknown but i see

you don't know anything about the system

we can only observe the system through

interaction

and given this interaction we want to

find a policy that performs well

where well is defined by maximizing some

reward function

and in deep rl typically this policy

takes the form of some deep network

it's a very simple definition of dbr i

guess but that's pretty

pretty accurate also

and the unfortunate part is if this is

genuinely your setting here you are a

little bit

uh out of luck because um

if the system is unknown you basically

need to take

unsafe potentially unsafe actions in

order to explore it

to the point where you can actually

learn what to do if you knew nothing

about the system

um you know you can't do much

if you're just given an airplane and say

you know i don't i don't know how

anything affects it

stay in the air you're probably not

going to stay in the air at first you're

probably going to crash a few times

before you learn standing here so

we need some middle ground between these

two things and so this is why we

actually are thinking about a modified

setting that is actually the robust

control setting

in other words we actually assume that

true systems is still unknown

but we know the exact same thing we knew

um

from the robust control setting in other

words we have

a set of you know linear matrices

basically or matrices here

they characterize the uncertainty of the

system in the way you saw before

so the system is you know a linear

differential inclusion

with some norm bounds that depend you

know with linear depends on these terms

here

and the normals depend on on these terms

here or some ordering of this

and the reason why this is actually

legitimate even though we're applying rl

is that this is realistic in many

settings

you know we know that we do in fact have

a description of the system and so we

should

use that in order to improve our

performance we shouldn't actually

whether you're controlling an airplane

or a building it would be crazy

to throw away all the knowledge we have

of these systems

and just apply pure rl uh as if we

didn't know anything about these systems

if it was just literally a black box we

had no we had no idea you know no

concept of um

and so i think there's actually as maybe

as an aside

um you know i think this is actually an

important thing to consider in rl

as a whole is is what are the

assumptions we make about what's known

and not known

and there are a lot of cases where we

actually can assume a lot is known

because we actually have that in

practice

with that being said though the key

question i want to address is can we use

reinforcement learning methods to learn

a better policy

than a robust control law

while maintaining robust control

guarantees and i mean that you know both

while you're learning the system so

while you're learning while you're

running your rl algorithm

uh to sort of learn the right control

policy and at the end of learning itself

and these are going to be the the this

is going to be the main

challenge that i'm going to ask here in

the remainder of this talk

so the answer as you might guess is yes

we can

um and i'm going to show you how to do

that

so here's the key idea

the key idea is we're going to first

before we run reinforcement learning

we're going to run

robust control okay so we have those

matrices

we're going to run our robust control

algorithm we're going to get out a

policy

you know we're going out the the control

the gain matrix k

as well as we're also going to get all

the yapino function

that guarantees stability of that policy

right under any disturbance

and then the cool part now

is we can actually look at this lyapunov

function

and ask the question what are the set

of all actions we can take for a given

state

that actually decrease the lyapunov

function

because uh and and here uh you

just see the paper for these details i'm

going to try to hide the main point here

but but um

this is this is actually probably the

hard the tricky math that you do have to

know a little bit so i'll kind of go

through it in some in some detail

this is not a a joke equation here

um so here's what i want wha what what

sort of the key idea here

yes so

when we have this lyapunov function of a

particular form and we have

the linear like you know the bounds on

the system

prescribed by this linear differential

inclusion

we could actually analytically solve for

what is the worst case what is the

largest

possible increase you could have in the

abinal function over time

and we must we sort of denote this as as

v dot that's essentially

the time derivative of the lyapunov

function itself that's how the

the alpha function is changing we want

that to be decreasing

and so if the maximum sort of increase

is uh is is greater than zero then

we're not decreasing anymore all right

so we want this thing to be less than

zero basically

and by the way like actually this sort

of form is the same way we derived that

great big

lmi before but uh i won't it's actually

simpler once you already have the

lavender function substantially

so this sort of expression here on the

left this denotes

um the worst case increase in our

lyapunov functions under any disturbance

and we want this thing to always be

negative

and if we apply some algebra it's not

very hard it's you know one line of

linear algebra or two lines of linear

algebra

um what we see is that we get a form

like this so it's some constant

plus another constant times some vector

plus d

times the this u term

plus this term here um is of course

these constants depend on the actual

state that you're in uh this this thing

is you know for a given state

but for a given state the worst case uh

increase mapping up function looks like

this

and the key part the only part is

actually important here

to note is that this worst case

increase is a convex function

of the control input has a term that

involves

a l2 norm a term

is linear but this whole thing here

which i'm just going to call

abbreviate as s for like safe use

um this whole thing here is a convex

function and that's actually a very

generic thing

i mean the reason why robust control is

possible is because that thing is convex

and so for any reasonable control uh

robust control discussion

this will be a convex function of the

control

all right so now we have the main sort

of challenge here

we want to ensure that whatever

our neural network policy outputs

satisfies the constraint that this thing

is less than zero

and if we can do that we actually will

guarantee

that our policy will also decrease this

lyapunov function

so that is what we do

so here's the slide sort of actually

showing what we do

so we're going to specify

our deep policy as something that

takes some arbitrary deep networks

output so this is like a normal neural

network policy would have it's just some

neural network function of the state

and just projects it onto this set of

safe states there's a little bit of

details here like

you can't actually have like something

that's less than zero strictly so you

have to actually have a little bit it's

a little bit strictly negative

um depending on the size of the state

but don't worry about that this is

conceptually what we're doing

um and because this is a convex function

the set of all points that for which

this is this is negative

in this case let's looks like a cone

here

um in fact in the previous case this is

a uh

you know norm term so this convex set

actually ends up being exactly a second

order code

so that's not an important

characterization

okay so graphically what this looks like

is the following we have some you know

some safe set here we have a set of all

controls

that are are safe in other words they

decrease the lyapunov function we know

there exists at least one element in the

set by the way because we know that k

times the state is in that set so this

set is not empty

it has some elements to it this is very

important actually and we know that

robust control tells us

an element of this then

the way we define our policy is we just

take our arbitrary neural network

function which very likely will give us

a point outside this set

right because it's there's no reason for

it to be inside the set it's a neural

network

it could definitely be doing something

unsafe almost certainly

and we project it onto the set

and when we do that now all of a sudden

this policy pie has all the guarantees

that the robust control policy has

because

that policy pi is still decreasing our

the epinop function and

sort of very cool

we can also apply

any reinforcement learning algorithm we

want to this new policy here

and the reason we can do that is we

actually though this is a little bit i'm

not going to talk about

uh okay this set only okay so so for

mario says it said only include no the

set's actually bigger than just that one

u

you can probably show this is this the

set has has none it's not a singleton

um it has i mean in the very least you

can have a scaling of that ut that's

sort of a cheating one right

but you can just like pick the scaling

of it uh that's pretty lame

um but it definitely is not a singleton

set it has it

it is a set that contains many elements

um many of which are going to be better

than the linear control law okay and in

fact what you really want to do is sort

of like

you know depending on your your your

actual state you kind of want to bounce

around this whole thing right you want

to bounce around this thing kind of

arbitrarily in a non-linear fashion to

have the best control law you could have

um and because of this uh yeah so so

uh so someone asked how do you determine

the projection functions the prediction

function itself is just the projection

onto this onto the set

and actually the projection function

because

it is dictated by the lyapunov function

that's determined the actual projection

part is determined entirely

by the robust control specification the

cool part here

is that we actually and this is some a

long line of work we have actually on

differentiable optimization

it turns out you can actually different

just like you can differentiate through

a neural network to train it with

backprop

you can also differentiate through

projections

they are also differentiable operators

and

it turns out you can actually treat this

whole thing

this whole network as one big black kind

of black box policy

that just happens to have its like last

component specified by robust control

um and uh okay so it's the set of safe

control the form is easy for pgd so just

to clarify like

we're not trying to use pg to find the

set of safe controls

we're using robust control to find the

set of safe controls

and we're going to use pgd to train the

parameters of the network

of f here which can be whatever we want

like f can be as nasty as possible

uh because it's followed by its

projection it will always project back

into the safe set of safe controls and

therefore

always lead to a safe policy no matter

how we train the parameters of our

neural network

using any unconstrained optimization

method

okay so that's that's the key idea here

so let me just summarize this because

this is now getting and and then present

very briefly our results

uh and then pause and then end for for

the end of the talk and maybe have

questions i'm actually not sure

do we have them for questions uh

afterwards or just during that we should

have a couple of minutes for questions

okay so it's a high level here's here's

what we're doing

uh okay what's up say action constraints

um

uh no so so the so so right so uh okay

state and action constraints right

so uh first action conference we are

assuming unconstrained

linear differential inclusion in in

theory but that disturbance term can

actually capture strict constraints

that there that you do impose on the

states or controls

um so you can actually have constraints

on those

sort of implied by the allowable

disturbances that's a different model of

than the simple norm value model that's

a different sort of robust control model

but you can have similar bounds on them

there

yeah you can do that um

yes so they are okay this is the point i

want to make so if you consider

nonlinear

dynamics uh yes they are non-linear

that's the point i kind of want to make

here is that

is that is that i'm writing it like this

but this is not an equality this is an

inclusion

w our disturbance term here can depend

on anything it depends on the state

to control anything in an arbitrary

non-linear fashion so this system here

can be unknown and non-linear

we just are bounding it by this linear

by this linear kind of over

approximation of the system

which is exactly what's under robust

control those systems are non-linear too

and everyone knows they're non-linear

we're just bounding it with like an

outer bound of a linear system

um and and yes because i'm safe using

all those matrices abs abg but also c

and d which are the norm balance yeah

all right so simple

uh summary solver bus controls solve

robust control number one

number two use the output of the rest

control law to define a safe set

or really a safe set of controls for any

given state

and define the rl policy as an arbitrary

deep network can be whatever it wants

completely unconstrained

but then followed by a projection back

to the safe set

when you do this the resulting policy

will have the same guarantees as a

robust control

policy but a better but can learn

ideally

better non-linear control laws

using rl methods and that's the key idea

here

so let me just give a few brief examples

here so

i i'm i'm not going to spend too much

time in this video i'm just going to

show you a table next but try to

highlight some of the main results

you can check out all the actual results

in the paper i'll have the link again at

the end of the last slide

maybe i'll place it in the chat as well

um

so we ran to declare simulated

experiments so we're just we're just

trusting these things out in simulation

so far um this is a sort of an initial

study

uh certainly um but we did it on four

different domains

uh it's just sort of random different

linear differential inclusions to start

with so just you know generate kind of

random a b

g c d matrices et cetera um but then

some sort of common benchmark task likes

like cart pull stabilization quadrotor

control as well as a microgrid

simulation

i won't go into details of those things

again those are all in paper

in all cases what we compare is we

compare how well the approach works

under normal operations as well as

under kind of worst case adversarial

disturbances

as governed by these norm bounds that we

have

um so okay i want to i want to highlight

the results here this is a big massive

table

um i just want to highlight a cup i'm

going to blank out some stuff in a

second and just kind of zero in on a few

things

so so um let me sort of mention what i'm

comparing here though

what we compare across these domains is

first of all

um sort of non-robust control so

non-robust lqr which is a standard kind

of control method

a classical control method as well as

non mvp here stands for for model based

uh

model based policy learning from nrl and

ppo is

as a standard rl method but these are

all sort of non-robust

methods both the classical control and

and uh rl based

we also have a robust version of the

classical control law which

again uses this control robust

specification

and then solves a an lqr like objective

separate from this

and we have sort of using our method we

have robustified versions of

these rl methods and so i just want to

sort of zoom in on one particular

line here and it's a little hard to read

here so

so i guess i should have explained this

more but o means an ordinary

operations and a means adversarial

operation so like the worst case you can

experience

under any norm under any um disturbance

to the state

and what we find is our two important

things here so we find that

first of all our robust kind of pretty

much across the board here

and this actually is true for all the

others you'll see as well so across the

board

our robust rl method under ordinary

operation

works better than the

robust like the classical robust control

method that's sort of number

one um does not work as well by the way

as the

non-robust i mean maybe lqr is sort of

weird here i guess this is an

outlier here but it does not work as

well as lqr um

but uh under ordinary operation i guess

actually that that makes sense to you

anything about it

um but it does work well but but what

does work better than all of them under

ordinary operation is rl

right so if you just apply normal rl

with an arbitrary neural network policy

under ordinary operations that works

quite well so it works typically better

than our kind of robust methods

again because it can do whatever it

wants it can kind of violate whatever

whatever constraints it wants and

violate whatever safety it wants

but the big sort of second point here is

that when

we do actually have adversarial

situations our methods remain stable

whereas traditional lqr will always or

traditional

either lqr typically but also typically

rl methods

will go unstable they will they will

basically blow up the system will

will fail hit whatever balance it has

etc

all right so again a high level

the nominal performance is better than

than robust control

and the worst case performance is better

than rl

and that's the kind of guarantee that we

have from these systems

um so with that let me

oops have my last slide and just say

um the take-home message from this

is that under a certain setting we

actually can

incorporate robust control constraints

into generic deep learning

but i think the work here is just

getting started to a certain extent

um first of all you know we're doing

we haven't really done anything real

here uh there's no real

experiments here we're just doing some

simulation and seeing how this really

works

especially in domains where we actually

deploy robust control

i think is really the key step to

getting this to actually work

but then maybe at a higher level the

last thing i want to kind of emphasize

is

i've set up kind of one middle ground

between robust control and reinforcement

learning

other papers have done other kind of

middle grounds and there's a huge

wide array of things that lie between

these two extremes

and so part of my kind of hope for

communities like this one like the

controls community

um and the rl community very interested

in both sides is that we find these

middle grounds

where we can kind of do both these

things together

so thanks very much uh our paper is

available here

code is also available um

would love to get anyone's thoughts and

opinions uh but thanks thanks very much

oh

uh and i'll answer a few questions now

from the from the from the

chat but i also mario whenever i need to

move on just just tell me

i'll text after that yeah so actually if

you happen to see a question that you

would rather answer

uh for everyone that's okay that's a

very easy one so the performance numbers

are arbitrary lqr cost matrices so

they're they're

the actual numbers don't mean anything

all that is the relative scales um

yes it can use uh it can definitely use

off policy methods too so again it's

just a policy parameterization so

as long as you know balance the true

system you can use whatever rl method

you want including off policy methods

ppo is can be done kind of off policy

too but it's

usually done on policy um and then how

expensive is the project is

the projection the the production is

actually very straightforward and we we

run it i mean it's

it's easily real time it runs in a few

microseconds um

it does slow down the training a bit

because it's even though it's fast it's

still not as fast as you know nothing

um so so it's it's uh actually

microseconds is probably wrong but but

definitely you know on the order of a

millisecond

um uh but it is not nothing but it is

very very fast because it's

solving a tiny like uh projection onto a

second order cone which you can do

really fast

with some alternating projection methods

and that's basically how we do it

um why dprl yeah i mean to do clear i'm

talking about rl in general right like

this could do with

any url method deep is just because with

dprl it's

like you have a more expressive policy

and it's traditionally been hard to

impose constraints on that policy

precisely because of its

expressiveness um if you have a linear

policy you can kind of already do this

to a certain extent and so

i'm really focusing on the cases where

you have non-linear control policies

great well um

we're we're going to have to move on but

i'm sure that there are

questions here that will continue coming

um i'll stay around it for a while and

i have to give an exam in uh 25 minutes

so i will

i will have to stick around if you can't

but but i'll stick around for a little

while and i better join that like 10

minutes early too so so i'll

be up for 50 more minutes and i'll

answer questions in the in the chat

thank you so much zico um i don't think

we could have asked for

a better and more centered uh

talk than this one thank you so much

thanks all right

i don't know how we do this round of

applause virtually i will do my applause

all right uh zeldan or whoever is next

over to you all right thank you

zico very awesome talk um

i i would have one question but i'll ask

it in the chat so we can continue with

the uh

program so we'll talk our first session

i will start the first session and i'll

hand it over to

session chair um lucas spanger

from uc berkeley and i'll ask all the

all the authors are identified so you

should have

access to show share screen and lucas

take it away

um hello everyone uh really nice to be

here today and and uh

thank you again to dr culver for an

awesome keynote

um so uh a brief some brief information

about me

i'm a fourth year phd student at uc

berkeley

i'm in the lab of costas spanos and

right now

i develop reinforcement learning for uh

for setting up

prices for demand response throughout

the day um

uh and so as everyone knows

reinforcement learning is tough and

dealing with humans is

also very tough uh and so the two

together

have been an interesting challenge um

but uh it's awesome to be in a workshop

that's geared towards

like the generally like a very similar

area

um the way that this will work is that

we have

um 50 minutes in total i'm gonna

introduce

our first our first authors um

from the technical university of denmark

um

and the presenter is

um is morton christensen

so um morton received his master's in

psychology

from the technical university of denmark

in 2010

after six years in industry as a

consultant and designer of

automation systems he returned to

academia in 2016

and he's been working as an industrial

phd fellow under the supervision of

professor pierre vincent

um working for more than five years

on the energy lab nordhaven project

gordon has extensive knowledge

of the role of flexible consumption and

intelligent buildings

in integrating energy systems

so in terms of morton's long-term

research interests

they want to explore context aware

control so

in some ways i guess similar along the

same lines as the keynote

um they're looking to control buildings

as a key source of flexibility

they aim to include knowledge of

buildings and documents into deep rl

based on controls to create building

controls that are better smarter and

greener

they love spending time with their kids

reading sci-fi

and of course the very little task of

saving the world

so uh with that in mind i'm gonna hand

the stage over

to morton and uh morton

gets seven minutes to talk and then two

minutes of q a

um morton please maybe have

my video pinned so that like i'll i'll

hold up a sign

saying like one minute left to try to

keep you on time

okay thank you very much lucas for the

uh lovely

introduction um uh and uh

i'm i'm very honored and happy to speak

today and it's a tough job uh

following the keynote that was that was

extremely uh

good and interesting um and let me

start by giving you a very high level

introduction so

uh the the energy systems are changing

right from

from a situation where fossil

based power plants schedule and plan

according to

known or static loads to a situation

where the rolls are flipped and

flexible loads are managed according to

the production from intermittent

renewable energy sources

so uh in in this talk i will present our

results

exploring two of these key problems

within demand

response the first being a price

sensitive

control of a single building uh

where electric uh electric based space

space heating is is made price

responsive and

sort of balances the cost and the

user preferences and the other is a

coordination task where

uh population of such price responsive

agents

are then controlled by setting a price

in order to manage

a total load in the electricity grids

so one way to do that is

through optimization and and here we

have uh formulated uh

a linear program with an objective

function that includes

uh the total energy cost

and a thermal disutility uh that

represents the uh the

the willingness or the cost

to transgress the the comfort bounds

of the user and

subject to constraints that represent in

a very similar manner the

thermal dynamics of such a building in a

in a linear

model uh when we translate that

to a reinforcement learning setting then

the

states we're looking at here are the

temperatures in

inside and outside the solar irradiation

in the in the house the price of

electricity and the time of day

the actions for the reinforcement

learning agent is then the

the heat pump operation and

in similar manner as for the linear

program we have

formulated the rewards based on

energy cost and the thermal digitality

so the nice thing about this of course

is that we can solve the

the the lp to a an

exact solution and and then we can

compare

to the performance of the reinforcement

learning agent

for the the bi-level program or the

coordination problem it's

it's a bit more tricky and we still have

the

the terms for the energy

cost and the thermal digitality but now

we have included a cost that

is the load violation digitality of such

a demand response

operator so sort of his willingness to

pay

in order to keep the load

l max between below some bound

the the thermal

dynamic equations of the building are

the same and and

then we fix the heat pump operation

to the output of the

reinforcement learning agents of the

previous problem

and then there are some constraints as

well on the on the

total load limits when we solve that

problem in

using deep reinforcement learning our

new states

are then the total unflexible base load

the temperatures outside and

inside in the individual buildings and

the

action for this problem is then a set of

prices

again the rewards are based on the

energy cost

as before and the thermal digitality and

including this load limit

disutility for the demand response

operator

going straight to to the results

we and the case study we compared the

performance of pretty much

vanilla dqn and ddpg to to

baseline which is sort of a naive

hysteresis controller and then the

the ideal optimal baseline

which is the solution of the linear

programs

for the binary and the continuous case

and when we

look at the results in terms of the

total energy cost

we find that for the nav

hysteresis controller performs worst as

expected

with the highest total energy cost and

the

highest energy consumption

whereas the the true reinforcement

learning or deep reinforcement learning

agents they perform almost as well as

the ideal baseline

so when we look at the the coordination

problem

we had just a dqn agent that

is compared to a static and a time of

use

pricing

and when we evaluate the results of

of those we see that

the reinforcement learning agents the

deep reinforcement learning agents

are able to manage the loads of the

population of

buildings better than the

time of use which is in the green but at

a

much lower cost

and the while the static

price is sort of efficient in in the

in in managing the total load it of

course depends on

on the price that you set for the energy

so in in summary of the the paper

uh we found that this

deep reinforcement learning framework

uh to be a a strong uh

framework for this model free control

that could potentially uh

release this flexible consumption in

buildings and while

the deep reinforcement learning agents

they

outperform our naive controls and

with the performance that is approaching

the the optimal controllers

our first step towards this full

multi-agent setup

solve solved the coordination problem

uh and in the lower level problem

uh of price responsiveness was solved as

well

of course uh in future work

we would like to explore the the full uh

simultaneous learning of multi-agent

setup but i think actually today we will

have some interesting presentation on

that exact

subject

and uh i think that was what i had

planned for my seven minutes i

i i'm i completely lost track of time

lucas i'm not sure

yeah yeah um

uh i i think it's you were

completely on time actually and uh and

now i think we have two minutes for

questions

um and so i think we would probably take

two and i'll limit each to one

in it

if no one else has a question um i i

have one that i'm very curious about and

this is more of like a

a meta research question and um it was

around

um how did you uh

how did you formulate the specific um

research question that you looked at

right now

like what what were the um what were

some of the

challenges in in trying to and trying to

map out the technical details

well uh i mean for me

the the reason why i think that

reinforcement learning has such a a

promise and i i liked a lot but

secret said and we shouldn't throw out

the the the knowledge of the buildings

on one hand but on the other hand for

for my application which is

uh i guess for flexibility the

the value gain is is quite the potential

value gain is quite small right so so i

really enjoy this

sort of black box or completely model

free approaches

because i'm not sure there's gonna be

people spending a lot of effort

uh to incorporate knowledge of a

particular building into into

the controls or i i don't know we will

see

at least for these small low value loads

something that is is more um

like go out and explore is is

is really appealing um

sure

um what led you to look at specifically

at water heating

well this is this is actually based on

the space heating of uh

using a heat pump right so so in in

denmark we're seeing

the electrification when we want to use

all this renewable energy then electric

heat pumps they are they're just a very

nice

thing in in that they they couples or

the the electricity system

to a huge source of flexibility which is

our our build environment right so

you can store quite a lot of energy in

in the buildings potentially

that we would otherwise have to buy

batteries or

other forms of storage for yeah that's

sort of

the the motivation behind that i i

i i agree and and as far as like

um a latent state goes um it seems to me

that

uh water water hearing heating is

particularly

well applied to reinforcement learning

that helps you unders

model these latent states um i have a

question from nicholas fotress

um nicholas do you do you want to ask it

yourself

or shall i read it um yeah sure

i can actually ask it if you guys can

hear me

yeah yeah uh basically i was uh

wondering if

uh the market prices were taken into

account uh in your

kind of uh reinforcement learning and if

they were trying to

adapt the heating strategy based on when

the market prices were highest or lowest

that's that's the lower level uh problem

exactly yes then that

that is what uh the the

the price responsive the behavior is

learned exactly using

spot market electricity prices

correct

all right well um please join me in

either a virtual

clapping reaction or a physical

collapsing reaction

um because for a fantastic presentation

and a difficult presentation of being

right after the keynote

thank you very much but i'm very happy

to have the

opportunity to learn with all you today

and uh i'm looking forward to the rest

of the program

um so um

now it's my honor to um introduce uh

a group from oakridge national lab on

electricity pricing aware rl based hvac

um it's an extremely technician paper

uh very thoughtful very well written um

and so

i i'm looking forward to the

presentation a lot um

so the presenter um

is uh

the presenter is is um is curved i

believe right

um and uh

so sorry um

uh kurt is a research scientist

in the computational urban sciences

group at oak ridge national lab

he finished his phd in image information

mining

from iit bombay india which is uh

out of the iits in india many consider

to be the best

although i think iit delhi would

probably contest that

during his phd he generated an interest

in fields of machine learning

in high performance computing for

various geoscience and remote sensing

applications

after his phd he joined oak ridge as a

postdoc researcher

in scalable geocomputation in january

his long-term research goals are to

contribute towards the betterment of

society by conduction

by conducting applied in computational

research and development

in urban sciences his specific research

goals include

conducting data-driven research for

urban mobility

geosciences approaches in transportation

and data-driven approaches for energy

optimizations and buildings and smart

controls

uh personally he loves

badminton cricket and reading books and

listening to indian bali music bollywood

music

and my my first question before the talk

even begins is uh

is uh who your favorite bollywood artist

is

thank you lucas for that uh beautiful

introduction so i'll start with uh

with your question so do you know

shahrukh khan yeah

yeah so that's my favorite actor

yeah all right let me share my screen

okay is it is it visible lukas

yep okay okay all right all right then i

can start

um hello all um i'm kulip kote i'm

recent scientist in oakland national

laboratory

the title of my talk today in rlm

workshop

is electricity pricing aware deep

reinforcement learning based intelligent

control

for hvac the specific focus of our work

is to explore

how well-crafted features from

electricity pricing can improve

observability

and the performance of rl agent so um

before i begin my talk i would like to

thank my collaborators

and also our sponsors us department of

energy for funding this effort

this is the outline of my today's talk

um let me begin with a few facts

so buildings are the major consumers of

energy in the united states uh with over

40 percent of total energy consumption

the international energy agency

estimated around 30

of growth in building energy demand by

year 2035

um as per energy information agency of

us

over 51 households uh total

annual energy consumption um uh in the

year 2015

although uh was attributed to hvac

so from this fact it is evident that the

intelligent hvac control can

significantly reduce the

building energy operation cost as well

as

help us to minimize the environment

environmental impact

by reducing the peak power usage

um so in literature uh for for last

during

last few years there have been many

efforts in in developing intelligent

hvac control

so we can categorize those efforts in

three

broad categories one is rule-based

approach where we

encode rules so this kind of approach

needs very specific and detailed domain

knowledge so that we can include

generalized rules

uh to perform better then the second

category is the model based approach

for that we assume that we have

understanding of the model of the

environment

in case of mdp we have transition

probabilities

kind of understanding examples are model

predictive controls and as i mentioned

we need

accurate building model to achieve good

accuracy and those

approaches are also computationally

expensive the third category is the

model-free approach which does not need

the understanding of the environment in

terms of model

um uh examples are q learning approach

and those approaches

learns uh using the continuous

interaction with the environment

um as uh keynote

also mentioned and i'm also reiterating

that the deep

neural networks showed human level

performance in playing atari games uh

in um in that and they showed that

result in the nature paper

uh they used neural network to

approximate the q tables

okay so specifically for the problems uh

with the continuous state and continuous

action spaces

dq and are showing promising results so

dq and performance depends on the

articulation of the problems in terms of

uh state

action as well as the reward we all know

that

in case and the state represents the

environment

or the state of the environment in this

case maybe building

so the cost of hvc operation depends on

the electricity pricing and the energy

used for the hvc operation so this fact

led us to hypothesize that the

electricity pricing related features uh

can improve rl's

ability uh to observe the environment

and to learn the optimal policy

one such feature uh can be next time

uh next price change time to the next

price change so in literature

uh electricity pricing uh price uh have

been used

um most of them used raw values and not

specific

and not much attention has been provided

to extract the

specific feature from the electricity

price signals which is the

focus of our work so now

since i let the basic premise on the

electricity pricing related features

in this slides i will describe the these

features in detail

so this st represent the state of the

environment

where t represents the time of the day

tnt to

represent the indoor temperature key out

represent the outdoor temperature

and specifically pt and the pl 80

represents the

uh price related features uh pt is the

current price and pla

represent price look ahead features so i

will be explaining this

price lucrative feature you with the

help of this diagram so at any time ts

uh p1 which is the first feature

represents the price

uh for the next uh time uh delta t1

until the next price change

and the t average uh t out average one

represents the

uh outdoor temperature during delta t1

uh similarly price

p2 represents the next price delta t2 is

the duration of that price

and uh we are also calculating the

average outdoor temperature during those

uh during delta t2 now along with these

six features we are also calculating

uh an index called ndpi uh normalized

differential price index

just to capture how

how the price is changing whether it is

increasing or decreasing and how by how

much magnitude

so this is the set of uh seven features

that we are

using in this work um so

using these state features we trained rl

model

so we use typical meteorological year

data for knoxville

uh and use synthetic price signal

and then we perform training in the

offline mode uh

with the help of simulation environment

more details can be found in the

in the paper uh we used june data from

team

tmy and then we used we ran the training

process for 50 episodes

um for at the beginning of each episode

we are generating random

synthetic price signal by randomly

selecting the peak start time

uh peak duration and the price value and

then

for validation we all we selected three

days from the month of august

from the same tmy data uh the sources

for the data

also are also available in the in the

paper

so this is the validation price signal

that we use to validate the performance

of the pre-trained model

so this slide shows the preliminary

results of the validation experiment so

we ran

uh validation for entire month of august

and also evaluated performance for three

days of the month of august just to

understand the day performance

so what we observed here is uh without

rl model without price look ahead

uh uh shows the zero to seven percent

cost reduction over baseline

uh whereas with the uh the rl model

which was trained using price look at

showed the

uh price cost reduction in the range of

12 to 15

over the baseline so those results were

promising um

and also we observed that rl with the

price looker tried to

pre-cool the house uh just before the

uh peak price so that can be observed

here in this

uh uh in the in the bottom part of this

uh of this uh figure okay

uh so those were our observation okay

uh so last slide uh so uh in in this

work uh we

uh try to understand the significance of

the price lucrative related features

for our rl training uh and we observe

the encouragement

encouraging results which shows the 12

to 15 percent of

cost reduction over the fixed set point

baseline but there are still more

uh need to do uh in terms of

understanding the relevant features out

of those seven

uh price look at features and then more

experiments uh

need to be performed with the time of

use price signals and their

real-time price signals okay so with

that uh i will conclude my presentation

and i would like to uh thank you for the

opportunity

and and i'm happy to take any questions

yeah i'm so sorry like it sort of turned

a little bit over and so i think we

really only have 45 seconds or so for a

question

i'm sorry yeah i'm happy to also chat

about the slack

i am curious why um

yt like your time which i assume is just

like an integer that integrates that

goes up is uh explicitly sort of part of

your space

base even though i assume the rest of

your state space is also dependent on t

and uh the t just maybe just serves as

like a linear

inner uh so

your question is the time of the time of

day that you're saying

yeah the state feature yeah you include

a state speech

feature t um that i assume is

is correlated to their other state

features

um and only serves to like fit a linear

up concrete yeah yeah i think that's

that's a really great point

you're very right in that comment

uh definitely the outer temperature and

sometimes if it is a constant price uh

if the time

time of use price those two features

then uh

will be correlated with the uh time of

the day

but if uh so again i will go back to the

uh concluding concluding remarks one

where we want to understand the uh

importance of the feature so out of this

set of features we can

we can experiment with more uh data

and try to understand which feature is

putting significant

impact on the operation of the rl

training

okay i i'm so sorry everyone i think

we're

i think it it maybe it's time to start

to move over to the other

other point but there are actually a

couple of really good questions in the

chat

um i apologize there's like three

different things that i'm looking at and

so i didn't realize that there's these

questions in the chat

um for the next two if people have

questions can you please

unmute yourself and ask them uh during

the question and answer time

otherwise they might be missed um thank

you so much

yeah but but but thank you so much um

i really appreciate it um

okay so um

uh next up is anjakin

and so uh

vickin is um a final phd student

at the energy institute at university

college dublin

passionate about using machine learning

and data-driven techniques to make the

world a more sustainable place

it's great just like the rest of us

fantastic

his research is looking at the use of

data-driven predictive control in

buildings

including both model based and model

free techniques

thereby harnessing energy flexibility to

allow for the increasing of renewable

energy penetration

in future smart grid the future

electricity smart grid

he's particularly interested in a

comparison of model-free and model-based

optimal control strategies

and the robustness and scalability of

the techniques over over heterogeneous

building stock

andrakin completed a be at university of

auckland new zealand

and is specializing in mechanical

engineering they're a wef

global shaper the president of the ucd

ashrae student chapter part of

the part of the

um ucd sustainable energy community

a leadership network member of the asia

new zealand foundation

and a rotary young leaders reward

recipient

um and if you're

ready andre kin you are free to take it

away

um thank you lucasville introduction

hopefully

you can see my slides there

uh we can see your slides absolutely

perfect

uh karakosho to you all uh my name is

andrew and kel grandland and like lucas

introduced

introduced me i'm a phd student at

university college dublin finishing off

my phd i'd like to acknowledge

my co-authors who are also part of this

presentation and work

so today i will be talking about our

submission to the city learn challenge

that was held earlier this year and the

aim of this competition was really to

develop design train and

tune a reinforcement learning agent to

flatten and smoothen

the aggregated electricity demand

profile for a district of buildings

and i guess i started off with

model-based approaches in my phd

but um i've been tempted into the world

of model-free reinforcement learning

and even after a year and i have to say

sometimes i still feel a little bit

overwhelmed

um so i just skipped a bit uh

introducing this because we've had a bit

of context already from the previous

speakers

but i guess reinforcement learning in

particular is very promising

because it proves to be an automated

control framework that can take

feedback from occupants which is

especially important in terms of thermal

comfort

and also its potential as a model free

and adaptive control of the buildings

and this is important because no one

building is the same in

the heterogeneous building shop however

if we look at literature today

most studies only focus on single

buildings and especially demand

independent price signals finally

literature also also shows that

especially in the building energy domain

that deep rl has often been limited to

discrete

and all low dimensional action spaces so

it's these which really motivate the

city learn challenge hosted by

uh university of texas austin and

lipsa and the aim of this competition

again was for this district of nine

buildings

to minimize these five different

objectives for the

uh microgrid essentially which included

uh ramping load factor

average daily peak demand uh the

electricity peak and the total energy

consumption

by controlling domestic hot water and

thermal storage cooling for these

buildings

so just elaborating it's an open source

environment

with the mix of commercial and

residential buildings with

pre-computed energy loads from energy

plus

uh this just shows uh the particular

archetypes of buildings used but i will

keep going

so in our research we decided to focus

on the soft active

algorithm now we initially did

experiment with

uh other algorithms which is suitable

for continuous action spaces such as

ddpg and dpg but we noticed quite

personal performance especially with

regards to the hyper premises

so we decided to settle on the soft

actor critic algorithm for this

challenge

based on its relative robustness that we

observed

the sac algorithm we use is essentially

an off policy maximum entropy um after

pressure algorithm

proposed by hanoi in 2018

so it's a policy gradient method

essentially you have

an actor which develops policy

and then you have a critic estimating

the value function

so we developed a pi torch

implementation based off

previous work used in different domains

in terms of the reward function we

assumed i guess virtual price based off

the total electricity demand for the

district multiplied by

a scaling factor and then the reward

function is a combination

of this virtual price multiplied by

the electricity consumption for each

individual building

and the summation of that now we found

that we had to apply

some essentially manual reward shaping

to help the agent essentially charge

the thermal storages at night and

discharge during the day

this is something i'll talk about a

little bit later as well

we show the hyperparameters that we used

after

a bit of a sensitivity study and we

found that we had

uh to essentially uh modify some of

these uh hyper parameters

uh on evaluating the agent compared to

uh

the training stage we used a centralized

agent with a complete oversight of all

nine buildings

and the state space was composed of

proxy variables such as month of day

day of week hour of the day as well as

some ambient uh weather conditions

such as solar radiation and ambient

temperature

as well as um states of the

storage devices

in terms of results we compare the agent

with the manually

tuned rule-based controller so a score

of less than one

indicates better performance and we were

able to achieve this after about 10

episodes of

training in terms of performance

objectives this resulted in an

improvement of about 10.57

averaged over the five cost objectives i

mentioned before

when applied on the same climate zone so

as this figure shows uh the curve in red

shows that we were able to flatten and

smooth

the aggregated demand to some extent

now the one of the focuses of the city

land challenge was also

on the adaptability of the reinforcement

learning agent

to deal with different buildings in

different climates so the chat

we were assessed essentially on uh the

performance of the agent on a new design

data set that we hadn't trained on

and in this case it features different

climates so this

shows the performance of the agent on

these four different climates

and again we were able to achieve

improvements of less than one

albeit moderate or mild improvements

over the um four different climates

so to conclude um and to provide the

contributions of this work i guess

uh we developed a soft extracurric and

it was able to reduce the five

cost objectives on the challenge data

set so we're highlighting the potential

as

a plug-and-play style controller for

buildings

for demand response now scalability is

unknown especially considering our

centralized

solution and this is uh something that

we will look

into with future work uh

a major limitation of this work is i

guess the manual reward shaping we had

to apply to get the performance we did

and i'd just like to add that the code

is available on github as well

in terms of future work uh together with

the beta team at polytechnic tourina

we're looking at a decentralized

approach instead to this problem

um and then also in terms of my other

phd research

i'm also comparing uh the reinforcement

learning agent

with a data-driven mpc solution uh for a

u.s department of energy

large office virtual building and that's

using co-simulation with energy plus

thank you very much and it's a pleasure

to be here and i'll be happy to see your

questions

yes thank you so much auntie kim that

was fantastic

does anyone have any questions

yeah hi oh go ahead

yeah so i had a question yeah thanks for

the very nice presentation

so yeah i wanted to know your opinion on

how

in the future there will likely be more

energy storage because of electric

vehicles

and how what impact that might have in

the way we coordinate the energy storage

of different different buildings and

settings yeah i guess um good question

in our case we did not really look at

the capacity of the storage devices

but i think one of the brilliant uh

aspects of the city learning environment

is

you can easily conduct sensitivity

studies or different studies

with different values of capacity and

look at the i guess

adaptability of the agents to deal with

buildings with different capacity

storages or

increased storage capacity so i think

that's definitely future work and it's

certainly um

i guess something we just didn't have

the time to look into

but i think yeah having the tools

available to

look into these research questions is

very important and i think we're getting

there in terms of

having the environments to do this

research

i'm very curious if i could ask what was

the most challenging part of this work

for you

yeah i guess for myself uh i'm a

mechanical engineer so

in terms of the computer science and

algorithm

algorithmic elements of rl i find them

quite

challenging certainly the training time

was

a bit of a challenge at the start but we

were able to get access to a cluster

here

at university college dublin and that

definitely helped speed up

training but i guess this was also

promising in that you know in this

particular case we found that 10

episodes was enough which

was quite a minimal i guess

computational burden

cool thank you so much anja kin um

please let's see a brief round of

clapping reactions from everyone on zoom

um and uh and i think we'll move through

our final perform uh

final presenter of the day which

ironically uh

anja kin um underkin's work is around

the city learn

environment and now we're going to hear

from the creator of the city learning

environment

um jose uh so

um jose is part of the group of dr nagy

um jose is a postdoc researcher

at ut austin working at the intelligence

environments laboratory

um and he does research

um in the field of multi-agent

reinforcement learning for demand

response

um which in some circles is a very

controversial field

he received his phd in civil engineering

at the university of texas at austin and

he has a background in building energy

management and power systems

he plays tennis and he hikes and i have

to say that

in addition to technical skills he has a

very important

scale which is being a social researcher

he's very responsive

and very very helpful in interpersonal

communication and so

i highly recommend reaching out to him

if you want to chat about rl

or his general work and so with that

take it away

okay let's see

okay can you see the screen properly now

here yeah okay yeah thanks a lot for the

the introduction

so yeah my presentation will be on

benchmarking

different multi-agent reinforcement

learning algorithms for building energy

demand coordination tasks

so yeah first a bit more about the big

picture so buildings account for

40 percent of the global energy use and

30 percent of greenhouse gas emissions

and in the u.s alone they consume almost

70 percent of the electricity

which is a lot of energy and even though

we need to reduce it it's not really

important

how much energy we consume but when we

consume it

and in this chart we can observe a

phenomenon that is known

as the duck curve which happens when

there is a lot of solar energy

generation

and even though solar energy generation

helps reduce co2 emissions

it also creates very sharp peaks and

valleys so in the daily load profile

which are difficult to satisfy with

other sources of generation

so the objective of demand response is

to modify

the energy consumption of storage

um in order to better align the peaks of

renewable energy generation with the

peaks

of energy demand and reduce the cost of

electricity and improve

the security of supply

however one of the problems is that

electricity is a commodity that has

a very high value to price ratio

and for which consumers will be willing

to pay between 10 to 20 times

its actual price in order to to use it

and this makes consumers in in general

and willing to modify their energy

consumption habits by much and

and they also want to avoid typically

having to constantly take decisions on

when or when not

to consume electricity so this is

these are some of the challenges for

demand response and therefore

the demand response should provide its

benefits in an autonomous way

and trying to reduce the

any any type of dissatisfaction by the

consumers

so in our research we implement uh

reinforcement learning

um so as well it's been mentioned before

and it's been it's a model-free

controller that can learn from

historical data

and also through interaction with the

environment which makes it more cost

effective

since we don't need to develop models of

the system that we are going to

to control and that's why why this is a

type of controller that we are doing

researching

and all the benchmark algorithms that

are going to present here

use the soft actor critic reinforcement

learning controller

that they explain it was explained

earlier

in a previous presentation

and so then in order to prepare our

benchmarking demand response scenario we

simulated

a microgrid of four different buildings

in citylearn

which is the open aig environment that

we develop

for the implementation of reinforcement

learning for

for demand response and urban energy

management

so the reinforcement learning

controllers control the amount of

heating and cooling energy that is

stored

in chill and domestic hot water tanks

which are provided

energy by heat pumps and by electric

heaters that consume this electricity

from the same feeder in this microgrid

and the objective of the controller is

to minimize the average

of a five evaluation metrics that we did

that we defined here in order to measure

how good or bad

the demand response is

and then we also define a number of

states

that are based on weather conditions and

the electricity consumption of the

different buildings

and also the reward function so we use a

reward function

that consists of the on the

multiplication

of two factors an individual component

that is based on the electricity

consumption of each of the buildings

and also a collective component that is

based on the net electricity

consumed by the whole microgrid

or the net load of the of the whole

network

and then in our paper we analyzed four

different types of multi-agent

reinforcement learning controllers so

they all use the soft actor critique

algorithm

and the same reward function and the

same states

but they they are different in the ways

they define

and coordinate the different agents in

this microgrid

so one of the agents that we tried was a

central agent

in which a single soft actor critique

agent

controls the actions of all the

buildings and knows the states

of of all the buildings so a single

agent knows everything and takes all the

actions

then we tested also the independent

learner approach in which we have a so

factor critic

algorithm for every building

and they only know the states of that

building and they only take actions

for that building and these agents act

independently of each other

then we try a different approach in

which well as

we know from the previous presentation

the soft actor critic algorithm has

actor networks that take actions and

critique networks that evaluate

how good or bad these actions are

so in this approach

is a mixture between a central agent and

the independent learners

so we have decentralized actors that

take the actions on different buildings

and we have a centralized critique that

evaluates

the actions of of all the different

agents

and then we also try the value

factorization approach

with qmix which is algorithm an

algorithm for which

i leave the reference here but basically

it's even more decentralized than the

previous one

so it has many different agents that are

decentralized and the critic networks

are also decentralized

but they are aggregated into a combined

critic network

to estimate the total q value for the

different actions

and then we tried we tested all these

algorithms

and we trained the controllers for 22

epochs and observed how the fully

centralized agent

performed the worst and the independent

learners

perform surprisingly well to be acting

independently without knowledge of each

other and then the value factorization

approach with qmix algorithm

and the decentralized actors and the

pseudo

decentralized critics perform the best

of

all of these so here we can see a trend

towards decentralization so

decentralization helps

improve the learning speed because we

don't suffer from the course of

dimensionality

the act the the agent doesn't have

to learn from all the states of all the

agents that we have

and therefore by decentralizing the

different agents

we can find the this a problem that is

known as the

course of dimensionality so here we can

see for a given

for a given uh two i think 10 days or

two weeks

we can see how they were able to flatten

the curve of demand

so the main takeaways here are the

decentralization improves scalability

improves performance by fighting the

course of dimensionality

and it's better overall when we have a

large number of agents

and we need to implement it in a real

setting and not just a simulated setting

in which we we need to learn very fast

and we cannot simulate the same thing

again and again so thank you very much

and yeah leave it open for questions

um could i ask a question is that

already okay

sure sure okay

so thanks jose for your talk and i was

wondering

well you were trying to to tackle a big

problem of demand response on a global

at least in a larger scale for example

you showed a plot of california but

remember correctly in

one of your first slides i was wondering

why is that city loan challenge

relevant for representing this overall

goal of

solving demand response in that setting

so so the main idea is to

well first we have a problem in

reinforcement learning research

um for demand response or for other

engineering problems which is the lack

of

standardization and is we have many

researchers trying to tackle different

problems with different types of

algorithms

so the main idea of the city learn

challenge was to have a same environment

or a same problem and that many

different researchers can try to tackle

so it's not about making this

environment as realistic as possible

because for that we would need to add

you know the power grid the voltage

regulation and many other factors that

are taken into account

into real-world demand response although

over time we want to

keep adding complexity to city learn and

include in electric vehicles or even

even some other some other aspects

but yeah the main idea here is to to see

how reinforcement learning

can can start to tackle this problem and

how many different algorithms can be

benchmarked and compared against each

other

in order to start taking steps towards

a towards demand response and

i don't know does that answer your

question

i know i think i think i get the point

of view i guess you could have to get

started somewhere though

i was i was wondering like i was uh

checking your papers and uh

your your poster on bill says last year

so um

but i didn't find no explanation why you

were your selection of buildings or your

building setup like device configuration

is

meaningful or is it like something

that's happening all the time or

that is the configuration that would

solve that problem in a larger scale

because it

would be applicable to a large number of

buildings for example

so again so we do this as a proof of

concept to try to

to see whether reinforcement learning is

able to start

to manage the energy storage whether

it's thermal storage or

whether it's batteries or electric

vehicles which we will start adding into

city learn

and and to be able to to see if it's

able to adapt to different types of

storage devices

or to energy consumption profiles so

it's not really so our plan is not to

simulate an actual

city in california and to prove that

you know right now we are not there yet

like into

into trying to to do it in a very real

scenario we are just trying to

to provide a setting in which that

researchers can use

to try different reinforcement learning

algorithms

and prove that they can work for this

type of

of systems if that makes sense

thank you yeah so

so something important is that right now

many buildings don't have a high amount

of storage

so so sure it's not maybe realistic to

assume that every single building

residential building is going to have

this type of thermal storage for cooling

and heating

energy but in the future

we will start seeing an increase in

storage either with batteries or

electric vehicles

and every residential building will have

will probably have one at some point

so we need an algorithm that is able to

coordinate all this

storage capacity and and i think

reinforcement learning is a good option

so we are trying to have a an

environment that allows people to start

doing research on this

and comparing their algorithms with each

other

and yeah in order to to prove that it

can work

in a in a reliable somewhat reliable way

um i thank you i really want to thank

jose

a lot um his work has provided the

groundwork for a lot of work including

some of my own

um and i want to thank everyone who's

presented and everyone who's asked

obviously everyone in here knows that

seven minute presentations and two

days are bad um uh in just in terms of

amount of time

uh but i think we're scheduled for a

break right now

um i'll refer to dr nakey

yeah thank you thank you lucas thank you

everyone um we are

we have made up all our time so we will

which is great

so nobody was you know listening to

empty sounds

um so i would i would suggest um that we

do take a break

uh just because it's scheduled and you

may need one um you know to

breathe a little bit and then we'll

reconvene uh five minute pass the hour

whatever hour that is for you in any of

your time zones

so it should be 205 in eastern 105

central and then

wherever you are in europe or asia i'll

see you then and then

if there is questions please go ahead

use the chat go ahead use the

the slack channel and keep conversing

and then

the official program starts at 105. i

mean five minutes passed

thank you

one lobby one hall in a building

and we deal we assume 15 minute control

slots every 15 minutes we send a control

to the thermostat

to change its a point the state itself

is uh

the past uh zone air temperature uh the

that is the indoor temperature in the

zone

observed over the past several slots and

the outdoor temperature also observed

several slots cloudiness

and cooling load in the building that

was observed over several slots

and more uh high level time of the day

day of the week

information and we also use forecasted

information such as what is the outdoor

air temperature going to be over the

next several hours and what the

cloudiness is going to be

with these state the action itself is uh

we discretize the set point

access uh within the comfort bouncer so

every time slot there is a comfort bound

during occupied period uh comfort bounds

are 22 20

22 to 24 degree celsius whereas

unoccupied is going to be

20 to 26 so we discretize those axes and

thermostats are points are based on that

access

and the dynamics itself is a zone

simulator we

built our own thermal simulator which we

use as our dynamics model

um our underlying dynamic model which we

don't uh necessarily we don't access the

agent doesn't have access to it

except that it engages with it as a real

building

and the reward itself is made up of

multiple components one is the

energy cost itself we you assume a time

of few startups and

you can see uh we i don't have to go

into uh each of these notations but the

high level it's uh

the cost per energy consumption

times the energy consumption itself and

the uh then there's a comfort cost which

is a

in a dollar cost per degree our

comfort violation and every time there's

a violation above

or below the minimum maximum and minimum

set point

comfort bounds that we are going to have

a cost incurred here

and the overall reward is just the

negative of the combination of these uh

costs the total reward uh this is again

a sequential control problem and i mean

we are representing it as a you know the

variant of bellman equation essentially

we are saying the

total reward is a combination of the

immediate reward and the future reward

defined by the current state and action

and the process itself is a partially

observable mock audition process based

on this tuple state action a dynamic

reward and

there is compactor gamma um and

[Music]

this is the pond dp and not an mdp if it

were if we were access to the internal

thermal mass of the building

that is at any point of time we know

exactly the temperature of the air

inside the building

all the objects into the building then

that's all we need

to define what the next state is going

to be based on the thermostat

set point action we take but that's not

something we we are measuring

we only have proxy measures like

internal zone temperature

and the various outdoor conditions over

the past several time slots

that makes it a positive problem

operation process

um yeah so here's the agent that we are

adopting this is the double dq1 pretty

classic

from 2015 paper so we have this single

zone simulator

and uh for a for any action it

gives a reward and the action the

current state and the next state

all of them go into this experience

replay buffer and we have

uh uh external triples which are used to

train the

q learning we also maintain a target

network and then we have action

processor

to shape the actions based on our domain

knowledge

and the cycle repeats so our original

fund

for the original pump depot dq and

performance over a baseline policy which

is essentially

the highest set point it came out to be

something close to 2.32 percentage

or five year period and there is

definitely room for improvement

uh i we believe that the complex state

and action space that we chose

could have led to learning issues and

that have to be significantly changed

and one thing that we know is that uh

cost is driven mainly by the difference

in

electricity price for our peak and of

peak if the prices are the same there is

no optimization here

there is no need to shift the load so

shifting the load is going to be an

important

aspect of this optimization and the

important information that we need to do

that is the time of the day

knowing the time of the day and the

corresponding tariff is going to be the

most critical

piece of information that we need to do

this optimization so we define this

restricted form dp essentially a

super trivialized state space which is

just a

focus on time of the day and day after

week and the action itself is not

multiple uh points in the bound

space but just upper and lower comfort

bombs and return the dq1

and we find that we get a about two to

three times

increase in performance gains which is

what you're seeing here

and just to look at how this performs it

does do a pre-cooling around midnight

it goes to the lowest temperature and

then around 7 am when occupants start

coming in it goes to the lowest

temperature for that period which is 22

and then on peak period tariff kicks in

we go to the highest

temperature and so it does the

pre-cooling pretty effectively just

using the time of the day

uh so idea the high level idea is that

uh the

the lower bound the gains that we have

are actually lower amount of

performance under the original palmdp uh

the one we show on the restricted form

dp in itself is a viable policy because

we don't

suppose we don't know what the uh exact

time

interrupt but we're soon gonna have to

uh mute

if we exceed the timing just just so you

know

okay so that's fine uh that's pretty

much it so the idea is

state and action space matters very much

uh

instead of just focusing on improving

the agent learning

it's always good idea to look into

domain knowledge domain heuristics and

define the state space and action spaces

that are consistent with our knowledge

and we do need to

have more study on what additional

stated actions are

can be added without challenging oral

learning and how much the value of

performance value is

that's uh that completes the

presentation thank you thank you so much

um given given we are um overrun on this

talk we will save the q a later for the

panel discussion and move on to the next

talk

thank you um so the next speaker

is uh gautam biswas

he is a professor in the vanderbilt

university

at the eecs department his research

focuses on deep reinforcement learning

unsupervised and semi-supervised anal

anomaly detection methods

and online risk and safety analysis

applied to air and marine vehicle as

well as

smart buildings professor

biswas please please

please start thank you very much i hope

you can all hear me and

see my slides so what i'm going to talk

about today is

uh developing a deep reinforcement

learning controller

for non-stationary systems assuming

buildings are non-stationary systems and

i'll talk about that

and therefore we might need the

relearning to keep the

performance of the controller in in sync

so my co-authors are vishek nag who is a

graduate student

working with us in computer science

and marcos quinones guerrero who is a

visiting faculty

at vanderbilt university i would also

like to

acknowledge our plant tops who are a lot

who is a who are allowing us to use

vanderbilt buildings as living testbeds

so so all these controllers that we are

developing we are

actually deploying in office buildings

at vanderbilt

so so just to get started uh you know

buildings are important and as has been

mentioned in uh

in previous stocks they consume about 30

to 40 percent of the energy worldwide

by so therefore reducing uh energy

consumption in buildings is important

but it's a complex problem

because we are having to balance between

energy reduction and comfort of

occupants in the building

uh so therefore it makes sense to try

you know to apply sophisticated

reinforcement learning methods

to try and build efficient controllers

for buildings

so buildings operate in non-stationary

environments for a variety of reasons

the internal operations of the buildings

themselves are non-stationary

you know because conditions of the

equipment in the building might change

faults might occur occupancy levels

might change unexpectedly and

environmental conditions change too

because yeah there are weather patterns

but where the patterns are often

uh violated so i show on the right

that you know whereas you could have

normal profiles of occupancy

certain events could cause those

occupancy levels to change unexpectedly

similarly

that could happen for the the weather

too

so given that we have to model building

energy consumption as a non-stationary

process which means in our case we are

keeping the

the states and actions the same but

the transition probabilities could

change the reward functions could change

but again in this case in what i present

today we are not going to

try to relearn reward functions uh so to

get to the

the details of our approach uh uh

so basically uh this is a deep

reinforcement learning we have

a lot of data of the about the energy

consumption of our buildings of

the particular office building we are

working on on campuses called alumni

hall

we used about six months to a year's

worth of data to

learn the initial controller of the

building

using a deep reinforcement learning

method deploy the controller

observe the performance of the

controller over time and when we find

that the performance starts degrading

we use recent experiences

to relearn or to modify the model of the

controller

of the system and then relearn the

controller so another point i should

make

is because uh because buildings are

considered to be safety critical

not in the sense of aircraft or

automobiles but

but you know the plant plant

managers at vanderbilt won't let us

deploy a controller to learn

online because you know there are no

guarantees that

we would not violate for example comfort

constraints so therefore

we've had to take a more conservative

approach of learning the control

of the model of the energy consumption

online

learning the controller from that model

and then deploying the controller

collecting experiences

so a whole process involves this initial

offline learning and then

not periodically re-learning but

relearning when we find that the

performance assessment

is going down our reward function

involves

multiple components one is of course the

energy component of the building uh

another another component

is uh trying to uh we are doing

centralized supervisory control so we

are

only controlling the discharge air

temperature into the building

the charger temperature and humidity

into the building

but we are trying to minimize the

workload of the

of the controllers or or of the

hvac units in the different zones by

by sort of trying to keep the set point

of the discharge here as close to the

set points of the

different zones so that they have to

operate minimally

uh so and and so that's the second

component and then there's a third

component which is which

is trying to keep the set points in the

individuals

or trying to keep the temperatures in

the individual zones as close to the set

points as possible

and and so we go through this cycle we

we

use uh uh a regression function to

determine the trend

in in the reward so when we find when we

find that

uh the reward is decreasing

significantly

then uh then we uh reinitiate the

relearning as i have described before

and the building we are working in i

said we are doing

centralized supervisory control so we

are controlling the discharge

temperature and humidity from the air

handling units of the building

uh they operate in two modes when it's

cold

it just heats the air uh but when it's

warm and humid then it cools down the

air and then reheats it

back to a comfortable temperature before

before discharging it and we

we honor these two modes and our

controller operates very well in both of

these modes

so very quickly then showing you uh an

example

here we've looked at uh when

unusual uh changes in the environmental

conditions

occurs so here is a case where in summer

there is an

unusual drop in temperature uh in the

in the outside temperature of uh and and

that causes for example the reward

function to decrease

uh significantly as you can see here uh

as a result of which

we then relearn the controller and uh

and then the the new controller then uh

it sets the discharge temperatures to

slightly higher values to compensate for

the

for the cold conditions outside and as a

result of which we find that there are

significant energy savings with the

relearned controller than if we had

stayed with the

original controller the interesting

thing though also is that

after a few days then uh the

the environmental conditions returned to

their normal

environmental conditions and the

controller again adjusts

to these conditions so so one of the

points i want to make about this

is the way we are relearning we are

relearning only from recent

experiences and but also including past

experiences so there is the the issue of

catastrophic for for

forgetting does not occur so so we've

run it on a number of uh situations

where the

weather conditions changed in unusual

ways so this was

during a warm period it's becoming cold

we've also run it in situations in

winter where

you had a new unusually warm days for a

period of time

and we've seen that our controller

adjusts and and is able to save

energy so so very quickly to uh

sort of summarize what we were approach

uh our approach uh we've used strand

analysis so

so to trigger relearning only when it is

required and then we have this offline

relearning approach where we relearn the

model from

recent experiences plus plus past data

and and then relearn the controller from

that deploy

again continue to uh to capture

uh experiences as we go along we've

avoided catastrophic

uh forgetting uh by by

doing this offline we are able to

uh sort of ensure that we don't violate

uh safety constraints which are mainly

comfort

conditions inside the building and we

have been successful

in saving energy so one of the questions

especially given the invited talk we had

today

is uh could we could be instead of doing

this

off offline or off policy could we do it

on policy by putting by putting in

constraints

into into the into the learning process

so that we don't violate for example

safety

constraints so that's something we are

going to work towards now

our control is currently deployed online

in alumni hall we are collecting data

about performance

and one of the things we want to do is

deploy this

this controller in multiple buildings so

one of the

future problems we look at is transfer

learning to do parameter tuning because

no two buildings are

alike so i'll stop here and i'm glad to

answer questions

um hi professor

um biswas there is a question from the

chat

um that brought two questions so the

first question is

for the offline learning phase do you

use historical

data to establish a simulator first and

learn it

with it or you directly use off policy

ppo

to learn from the data no we we learned

from the model so we learned the model

the dynamic model which is a i didn't

talk about it it's a

lstm and then we learned the policy from

the model

and and the advantage of that is then uh

you know the model sort as i was

saying it does we relearn or learn in a

way such that there is no catastrophic

for getting

so therefore the controller we design is

likely to be more robust and

able to handle different conditions

um i think since we're running late we

will save the second

question for a week you can address the

question in the chat and we'll

save more questions for the panel

discussion okay thank you very much

thank you so

for the third paper

and the session we welcome dr

david bill gianno he is a research

scientist at the

computational science center at the

national

renewable energy laboratory his work

combines optimization modeling

artificial

intelligence and advanced computing in

applications to

computational problems and clean energy

hey thank you can you see my screen okay

yes okay thanks so much thanks for the

introduction and thanks for having us

um again my name is dave biagioni i'm an

applied mathematician

um at the national review of energy lab

and the computational science center as

we just said

and we about a year and a half ago

looked at comparing

model free and model predictive control

methods for price responsive water

heaters

um i'll preface this by saying i'm not a

energy management or building specialist

but we did work with some folks who are

at nrel for this paper

so i'm going to kind of present this

more from the hey i'm an applied math

person with a big computer

and want to know whether it makes sense

in this context to use

um optimization modeling and mpc or

these model-based kind of

with the timer sort of emerging model

based model-free methods

um that could leverage supercomputing

and so i will sort of talk about that

towards the end

um the question of why water heaters why

do we care about water heaters for this

study

well turns out water heaters are

relatively untapped source

of flexibility typical u.s household

uses about five megawatt hours annually

this is based on an online calculator

that um you can there's a link down in

the slide there you can see where that

number comes from but

basically these are devices that use a

fair bit of electricity every year

for a given household and we can think

of water heaters from a certain point of

view as a one-way battery that is

it's a device that can store power as

heat but can't return it as electricity

stored energy is rather used to optimize

end-user utility

we jokingly dubbed this the cold shower

cost

which i'll talk about more in just a

minute so the question we wanted to

answer with this study was given a

deterministic but dynamic

time of use price could we exploit

the flexibility inherent in a water

heater to support the grid while

avoiding unhappy users

and honestly the kind of more important

meta questions i think we were really

interested in this is kind of in our

naivete of

about a year and a half ago we were sort

of new to thinking about

some of these modeling and control

mechanisms was like what did we get

by using a model is there any reason to

go model free what are the advantages to

doing a model free method when you have

potentially a reasonable model of your

system that you could use in npc

um we also wanted to know could we

leverage advanced computing

to directly attack the sample complexity

that comes with doing model free stuff

so those were kind of i would say the

more important meta questions we were

hoping to answer with this which i think

we were able to do to some extent and so

as far as the the modeling and

assumption the assumptions we made with

this model we

um basically took the approach of using

a

error-free model of the device itself

and so it turns out

um basically identified a model that was

linear in the physics but has binary

on off control with the deadband being

modeled so the deadband is illustrated

there on the on the right of the slide

deadband is a you know it's kind of

meant to make sure your device

thermostatically controlled device

doesn't wear out by turning on and off

too rapidly

and so this actually introduces some

complexities in the dynamics that are

not

necessarily there in many other

household devices

but um so we use this error free model

for the device operation

we wanted to really really include

stochastic um disturbance in this

this case the disturbance comes in the

form of water demand

there are many ways you can introduce

disturbances for water heaters operation

but

we kind of focused in on just this one

particular one because it's clearly one

of the most important ones for water

heaters when

when is hot water being used we don't

have a necessarily a great predictive

model for that

for the purposes of mpc and so our

philosophy here was don't let the

physical model error confound

the comparison that we're trying to make

but rather focus

on the importance of exogenous

uncertainty in the form of water demand

the mpc forecasting and the look ahead

horizon how those things affect

control performance compared with the

model free methods

and so in the spirit of the clash of

algorithms like kind of

pitting these two approaches against

each other here on this slide

the npc approaches we considered we

looked at actually a

number of different ways of doing mpc

for this problem

uh we use gorobi as the optimization

software to do this so it's a really

good performant

you know commercial grade solver and i'm

not going to walk through the details of

the math here basically we're modeling

um we're allowing us ourselves to mod

everything from

perfect forecasting so npc which knows

exactly the physics and exactly

the exogenous demand um

but over a limited look at verizon all

the way up to a two-stage stochastic

program which is kind of reflected here

in this

p superscript n and the objective

function

and indexes scenarios and so we actually

did everything from perfect forecasting

with a single scenario all the way up to

multi-scenario forecasting two-stage

program

stochastic programming we also model

these kind of more complex looking

constraints down here at the bottom are

there to model the dead band which is an

important part of the dynamics of how

that

the water heater operates so

we compared this with our model free

methods and i i know it's

not totally fair just to drop these up

in the the notation up here in the upper

right of the slide but

this is kind of the rl formulation that

we used and we used

a library called rl lib which used

tensorflow under the hood

to examine the

basically using ppo which i think

everyone on this call is probably as i

mentioned many times already just in the

workshop today

with another black box optimization

algorithm called evolution strategies

which really does direct just direct

policy search over the parameter space

of the neural network mapping

between actions um and the episode costs

and so this not strictly two rl methods

although

yes is often called rl they're actually

two model free methods one of which is

reinforcement learning the other one is

really just black box optimization

and so i'm not going to go through the

details of these um

plots other than to say that you can if

you stare at them long enough you can

convince yourself

that even with perfect forecasting you

need quite a long look ahead

uh horizon for npc to converge on the

quote unquote optimal solution

optimal here being defined by an npc

model with unlimited look ahead

and no error in prediction and no error

in the model physics

um and so you even even if you have a

perfect forecast

of the exogenous disturbance you

actually need quite a long look ahead to

to do the right thing and this is

attributed as i'll talk about in a

minute

to the fact that there is a time of use

price spike or a peak

in the late afternoon and if you don't

look far enough beyond that peak you

can't really plan around it correctly

um it takes quite a bit of time to solve

these optimization models in the sense

that they are

mixed integer linear programs and as you

add

more time steps to your look ahead and

you add more scenarios in the case of

the stochastic programming formulation

you basically are getting exponential

growth in the complexity of solving this

mixed integer linear program this is

just part of

solving such an optimization problem

even with a good solver like aerobic

you're going to see this

you're going to see this happening the

takeaway from this the

overarching takeaway from this

comparison was that model 3 methods

demonstrated on average better control

performance over a held out evaluation

set of days and are much faster to use

online orders magnitude faster

this is owing to the fact that

evaluation of the policy

for these model free methods is

effectively a forward pass of a neural

network which can be done very very

rapidly

and so um i was personally interested i

know we got some

feedback from reviewers these these

pictures had way too much going on i

totally agree with that

um but i really wanted to understand

what it was

that reinforcement learning and yes

we're doing

that um is so different from mpc what

are these things actually doing

differently what do we

what's the agent actually learning to do

so we drilled down to two different

scenarios one was a high peak demand day

where um you kind of see the right um

the the peak

power price is indicated by this gray

bar um and there's actually

quite a bit of demand occurring um as

indicated by these gray spikes

during the peak price period

and so in this case npc actually does

better than the model 3 methods because

it's predicting demand to occur

as planning around it's willing to take

a little bit of

expensive power during this time to

avoid accumulating cold shower costs

which you can see

yes actually allows this to happen you

can see in the middle pane

this growing sort of um blob of of

orange spikes indicates the cold shower

costs are accumulating now because

yes agent is allowing the temperature to

drop to be quite cold

and users are getting cold showers i

want you to know your overtime

um okay i will just i'll wrap up by

saying

what we learned is that um evolution

strategies which by the way we trained

on about 1100 cpu cores in about 90

seconds so we really were able to train

up a very fast

um uh policy it basically learns to

preheat

before the peak price and then turn off

during peak price

and so is that a useful strategy i think

it very much depends on the nature of

the data and what you expect to really

see happening in the system

but we were able to kind of hone in on

the fact that's what the agent was doing

whereas npc first had a more nuanced

sort of scheme

depending on how how good the

predictions were that it was making

about the

water demand so with that i will

stop talking and say thank you very much

for having me we'll take questions in

the chat or whatever is appropriate

thank you thank you dave um i think we

will move on to the next talk

and leave the questions for the final

panel discussion

and for the next talk it's a paper from

pnnl

and the presenter is junyong lee junion

we

is a research scientist at the

bioinformatics and data science team

at the pacific northwest national

laboratory

he currently works on deep reinforcement

learning based electrical

load forecasting and reinforcement

learning based building control

for the pnnl transactive control project

june you can take the floor thank you

can you hear me yes i can hear you

[Music]

okay

can you see my screen as we can see in

screen

all right so um thank you for kind

introduction

about me and uh today i'm gonna

i want to share our take a preliminary

work at pnnl and uh

we are working as a team so we have a

brilliant team member here

so they are experts on uh building

simulation and doing the control

and machine learning i'm i'm pretty much

newbie a building area so

i learned a lot of things from these

guys

so uh as you already uh many many people

already mentioned before the deep

rl has achieved a great great success in

diverse fields

so as you know such as the gaming ai

robot control autonomous vehicle and so

on

since rl has been highlighted as an

emerging

control technique art has been

demonstrated

its potential to improve building

control performance

as already mentioned in other talks the

diagram shows the

typical plot floor of the rl based

building control for the building

control

rl controller has been applied to

generate the series of optimal control

actions by

running on optimal control policy

from interacting with environments with

multiple dryers

and errors here x

t represents the measured state

information at time t

indicating the building conditions such

as the room temperature

air flow and so on uh rl

agents model usually uh the deep neural

electron model

nowadays represent the control policy

model as a conditional probability of

control contractions by given states

which is to generate the optimal

contraction

u with the given state x

t or a series of historical states

and then we we should measure the reward

from the environment and the measure uh

the state transit by the given action

so using the next state

we can select the action again and

iteratively

we can update the parameter of our

models

to maximize the long-term rewards

first here i i i just try to

we just tried to implement the rl

algorithms by

ourselves to test the aria-based control

for our system

and we realized that it's it takes so

much time to implement everything from

scratch

and it's not easy to find something

unfortunately find something that we can

immediately apply to our applications

since there is a tight coupling between

the

already implemented rf frameworks and

building models

so here we just want to

implement the rv based building control

framework

more flexible enough to quickly

prototype

and test the arrow controllers for

target buildings

in various conditions by decoupling the

rs systems from the building models

it's not fully decoupled yet but

we could pick a full building systems in

terms of

these usual parameters of course

users can customize the gym environments

and building models

at the same time gene environments are

designed to

allow users to customize the control

variables and

observations for the target building

user users also can select

the target time frame for example like

days of years

for simulation moreover our user

can customize normalized function and

reverb function

and our optimization algorithm so

we are basically employed the open

source python packages

like open aig

and open ai baseline without

reinventing the wheels and we also

employed the high fidelity

hybrid energy plus modelica building

energy model using

the functional markup interface which

enables to

exchange the information between dynamic

building models

and everything is can be deployed with

the docker

image

so to demonstrate the proposed

the flexible framework uh i was asked to

test uh the reinforcement

running based controller for the typical

large office

building at chicago area so it has

the two 12 identical floors with the

five zones

on each floor and hvac system

has one chilled water system and one hot

water system

and 12 variable air volume system for

each floor

we want to control the chilled water

temperature set points

in three discharge

air temperature set points for the

bottom floor

and middle floor and the top floor

observation parameters include the in

our

in in our case study the uh observation

object parameters include auto

temperature

supply and temperature and then and so

on

so far and and typically for

the thermal comfort we measure the

predicted percentage

of this uh satisfied uh

shortly like a ppd value

so so

uh the here is a we formulate the real

function

by incorporating the energy conservation

and thermal comfort as shown in this

equation

for calculating the whole building

energy consumption we consider

the oval gas consumption hot water pump

power

chilled water pump power power

consumption associated with

two coding towers and condensers

and so on for the case study we set up

the weight

values same as one

and second term parallelize the average

occupants discomfort in all the zones

like this

so this figure shows how to

the rl algorithms like the pp

we imp we applied uh ppo

and a2c uh they performed

for a five day period during two summer

months

june and august in chicago area

and our air-based controller are trained

using the rf frame

using the ra framework we developed

and using the environment for the

continuous action space

baseline control here represents that we

are using

the manually tuned temperature set

points without

ra based control so

ppo2 is a proximal policy optimization

algorithm

shows the a little bit better

performance with the same

uh rl agent euro-electron models

uh at the like a optimal episode

uh the episode with the maximum the word

uh for the given ra model

uh a to c and ppo2 model improves

on the baseline values of the total

energy savings

we observed that at the optimal episode

in june

especially the 4.4

of the total energy consumption

reductions

for the august the reduction in energy

saving and

marginal improvements in average ppd

for ppo2 are 2.2 percentage

and 0.21 percentage

so uh and also we observed that the sum

of the contractions converged

to the baseline control parameter values

actually

here this figure shows the average

energy consumptions in various sub

components

between baseline control and ra based

control

in this figure we can observe that the

chiller power

is significantly reduced

in ppo2 case compared to the baseline

case

which is part partly partially the

time okay

compensated by the increase

so yeah so this here is my conclusion so

we

implemented and demonstrated the

flexible area-based

building control framework using the

open source

packages and then we are still working

on

the incorporating the high performance

optimization and following

modules thank you

any questions yeah um

thank you jun um there isn't

a question from the chat but i have a

question

so i think the the simulation

environment you created is very

interesting and when i was reading the

paper i didn't see a

github link for like accessing the

your environment is it going to be made

publicly available

yeah we are planning to make it

publication in public and then

but still i as i said we are still

working on that so

uh it's not yet fully coupled uh deep

coupled

so we are working on uh make it

uh fully thinkable hopefully

so um now we will open up the floor

for a panel discussion and

for the audience if there is like some

research quite generic uh questions

related to the session you can put it in

the chat and i

will read it from the chat so to kick up

the

discussion the first question i would

like to

ask is that since this session is on the

clash of algorithms

and in all of your work you're exploring

different

um different algorithms so what do you

see

as some of the gaps for applying

reinforcement

learning algorithm into building control

application

is there anything specific in this

application

that requires more research or

that is not directly transferable and

just to clarify this is a question to

all

of the presenters

yeah i can jump in i you know i really

enjoyed the the keynote

um presentation because i think it kind

of

helped frame some some questions that

i've i've had

and and i propose actually ways to solve

some of those questions some of those

problems

that is you know it's really common at

least at nrel it's very common to

encounter

research teams that have a simulator and

also an mpc model or some other control

optimization based control model

just to control that system and how you

can kind of take that

in a way it's sort of like a prior right

i mean if you're thinking about model

free reinforcement learning you have to

learn everything by interacting with the

system

npc models can be thought of as kind of

a kind of a prior the question is how

you get that information out

of that model to maybe warm start or in

other ways and form a reinforcement

learning based

method uh and also ensuring safety which

i think was a really cool part of that

opening talk so i think for me that was

a really

i'm actively really interested in that

question right now looking for

opportunities to work on

problems like that because i think in

real world systems you're going to find

you often don't have to start from

scratch

if i could add to that i i fully agree

about the two points you made those are

issues that we are facing in our own

work too

but uh recently there's been some very

nice work

on physics guided you know deep learning

where you can incorporate say physical

constraints into an

lstm network so that's something that we

are thinking of

investigating for example you know since

we are

modeling energy we have to make sure

that energy cons the

conservation is satisfied and there's no

guarantee when we learn a lstm

model that that is true so so so we are

we are looking into ways in which we

could do that but uh yeah those are

interesting challenges that we are

facing in with working in real world

situations

in addition to the fact that you know we

are working with much

in real high dimensional space too so so

that's another issue

you're muted uh does the other two

presenter

have anything to add on and their

experience

working with like testing out testing

out and comparing different algorithms

uh basic so can i say something let me

let me jump

in so um in my experiences when i just

tried to apply

like a multiple different arti-based

algorithms and then i realized that

depending on the problem actually

the day shows a pretty much different

behavior

so uh the another question is how we can

uh optimize the hyper parameters and

like the rl uh molded structures

so because when when i just try to run

our simulation like five day simulation

uh

it takes so long time and then when we

think about the real

world problem so when we try to use a

real reading system and then

how we can uh you know solve these

issues

how how we can how can say the speed up

the to select the hyper parameters in

uh the model structure

quickly something like that

that's really challenging to me

so just to add to this uh i wasn't able

to

make my concluding remarks

let me let me use this opportunity

i think i agree with the many of the

comments now and particularly professor

this was right

the physics-based models i mean we don't

have to even go into sophisticated

ways of incorporating physical

constraints into a deep neural network

like lstm

in fact if we know that there exists

some heuristics that the

domain experts have already always been

using for several years in the company

or in the in the community and we know

how those heuristics are performing and

what kind of states they are using

that could actually be a good starting

point just take it from them

and define our hyper parameters based on

that and then we can use that as our

baseline and build on top we can con

we can add more advanced uh state

definitions and see how the performance

improves or sometimes it can degrade

because you know it gets more the

the loss function becomes extremely

complex for an oral agent to maneuver

uh we can take it one step at a time but

going back to the basics

talking to domain experts and taking

their insights on defining some of these

hyper parameters can be a pretty good

low hanging fruit in my opinion yeah

i i think the it's i think the

consistent theme that all the presenters

agree on is that

adding from like model-free

reinforcement learning algorithm we

should be

incorporating domain knowledge

from our building control community so

that we can

make these algorithms more simple

efficient

along with more robust and safer

the other question there is one question

i think is very good from the chat

that um i think it is related to

what we are comparing algorithms what do

you think how would you select the

metric for comparison

the specific question in the chat is

that

in the session there are four different

papers and four different

paper opt for four different

comfort metrics so um

how so i think the important question is

that

how do we have a consensus on

evaluation in general for reinforcement

learning

algorithms and specifically in this

context for

comfort evaluation

so it's like two questions one is on

evaluation in general what do you think

are

the correct evaluation metrics and

procedure to use

the other is more specifically with

respect to

comfort and how we as a community reach

a consensus so that

we can benchmark our work effectively

i don't know if we need to have a

consensus i know that for benchmarking

it is good but if

coming from the industry different

customers have different requirements

some customers say it's in hospital they

are pretty particular about temperature

comforts

but some customers may not be as

particular so

the way you translate that into an

optimization it can be either a

constraint optimization

where you have heart constraint imposed

or it can be a weighted

optimization where you just give some

penalty for violation

so i don't think we can even come to a

a single magical metric uh

for benchmarking purposes i think the

only thing we should

strive to is the algorithms that we

develop are directionally correct

no matter what specific formulation we

use for these comfort moms

we are directionally going in the right

direction that's a that's pretty much uh

how we should uh try to enforce it and

for that we should

try our our agents to

optimize multiple versions of

the same comfort violations try try to

have

evaluate them for various choice of

comfort

metrics and hopefully in most of them

are all of them

the agent that we're building uh can do

a pretty good job

you know the the challenge i think uh we

face to some extent

is sure right everyone in a building

you know needs to maintain comfort

parameters

but at what level of granularity do you

do that do you go into every room and

make sure that

you know whatever a person desires is

what you can meet

or do you do it by zone or do you try

and do it in some average way across the

whole building and and that that's

that's something that

you know we have been thinking of now in

some cases we are lucky because

especially in vanderbilt these buildings

are very well instrumented so we have a

lot of data but

in many cases that may not be the case

so even though you may want to

you know manage things at a certain

level of granularity do you have the

data

you know to to to be able to do that so

so these are these are really

interesting

challenges and in general i think reward

functions are

should be determined by the goals and

you know

not not all of us all all the time i'll

be working on problems that have exactly

the same goals

i'll just throw in from more of an

applied math perspective that

you know if you have a hard constraint

on your system i've seen some

interesting recent work on

kind of using dualized methods where you

kind of solve reinforcement learning and

then dual

do dual ascent type updates to price

parameters or the grunge multipliers

in your objective function that enforce

hard constraints on the system

in that case you don't have to be

ambiguous about what that weight should

be there's an optimal value of it

and there's some interesting theory

behind that too i think recently

developed

that's very different than trying to

attribute what a person or

user actually values for whatever

resource it is they're using

that to me is a much more difficult

question that i think often just comes

down to a modeling choice

um i would love to know there's a more

systematic way but that seems to be what

i've always found myself

uh facing when it comes down to choosing

those numbers

yeah i agree i think it's not really

important to

to know what metrics we use to define

comfort or

or what we try to optimize but also the

relationship between them

like how much weight or importance we

need to comfort with respect to the

energy

because sometimes we focus a lot of

energy

but yeah if you take a hospital that you

were talking about before

then sure the energy consumption is

probably just a small percentage

of the total cost of that hospital while

salaries or

other costs are much greater so you just

want to prioritize a lot

to make sure that people are comfortable

and maybe for the building owner

energy savings is not maybe the main

the main question necessarily so by

properly

taking this into account maybe it's

easier to convince people to implement

energy savings controllers into

buildings by making sure that

that we can guarantee that the comfort

is always satisfied

um i think the uh

the common theme here is that um there

to summarize the common theme here there

is

um a very user specific

and customized definition for comfort

that is

usually like based on specific

building stakeholders requirements uh

that resulting in a difficulty in

basically defining a universal comfort

metric

and also another consideration as

they've chipped in

is basically how you define your comfort

will

uh modify the difficulty of how you

solve the uh

the optimization problem so these are

the

two considerations with respect to how

to define the and also to add on the

third point jose is saying

is that how much we should incorporate

how much should we weigh

comfort into um the balance with

energy so i think this is so

um all these taken into consideration

this

kind of explains why it is hard to

define

a universal um comfort metric despite

the need

that if we have a single metric it is

easier for us to compare work with each

other

so um i think

i think we had a very good discussion

that we have

seen like common themes chipping in

and i am not going to overrun too much

to take a helios session

so i'm going to wrap up the discussion

over here now

but then we can still use the chat and

also the

happy hour to continue the discussion

thank you everyone let's take a five six

minute break and reconvene at 10 past

and then we'll start with the last

session thank you

thank you

okay thank you okay welcome back

everyone let's

start the third session of the day i'm

keeping it real

my name is helia zandi and i'm sharing

this session along with um

june park as a culture so we're gonna

have four presentations here

each of them nine minutes long about

three through

six minutes through the presentation i'm

gonna interrupt you to just let you know

that you only have one minute you don't

need to do anything

just ignore me but it's just to let you

know that you have

only one minutes left and just we can

respect the time and have a good

final discussion so i'm gonna

start with the first presentation

which um on which lucas spenger is a

presenter

so he is a fourth year phd student at uc

berkeley

under um costas panels which means that

he is about one year away from the

question when will you be graduate to

become

existentially troubling he tries to

develop reinforcement learning for green

energy controls broadly

in the past he has performed larger

scale modeling at the department of

energy advanced research project

agency arpaio where he plans to return

as a program director

liuka's long-term research in interest

are enhancing financial invest

investment into advanced screen

technologies green energy is more than

just a job but a passion

lucas has a tattoo of a win power on his

back and a tattoo of

concentrated solar panel on his arm he's

pretty um he's the president of his 50

percent on person grad

student co-op and an active member of

the radical furry community in san

francisco

check out his column on green energy at

the huffington post

on medium.com so because

the floor is yours if you want to start

a presentation

yeah just uh really quickly this is uh

the concentrated solar on my arm

you can probably guess which tattoo cost

more per kilowatt hour

i mean per square inch of ink um

just kidding anyway uh here

is my presentation um

so uh

um my presentation is entitled

um you can see you can see my screen

right

uh yeah okay augmenting rl

with planning for optimizing energy

demand response in a perspective

experiment

so the purposes of these talk are to

introduce office learn as an environment

for rl to introduce an rl agent that

does well

uh and to talk about my future clap

future directions so i can invite

collaboration

and so this is a lot and so i'll try to

go very quickly

um part one framing i think everyone

here is aware

that um like sometimes renewable

energies generate too much

and sometimes grids turn this just turn

this off

and i bring this up because i'm not sure

if you if you know that california last

year

showed off three percent of its

renewable energy resources for this

reason

some days up to 30 percent so this is

bad

but demand response is a way to deal

with this in which we offer prices

throughout the day

to incentivize shift these prices are

calculated with market supply and demand

they are not behaviorally behaviorally

optimized

so our question is can we do this

can we make it an ai agent that actually

learns how to make the prices better

and make them behaviorally optimal

so they're part one office learn an open

ai gym environment

and shameless copying and plug goes to

the navy group

for their citylearn um name

um so uh this is the flow of information

that our agent goes through and i'll go

through each of these boxes very quickly

the state space so the controller takes

in great prices throughout the day

it can also take in yesterday's energy

and some other details according to what

you want

our environment currently supports time

of use pricing and real-time pricing

with a day of prices and a year of

prices

it outputs a transformation of these

same exact prices

so a1 through a10

uh are different values between one and

ten and they have um

and they have uh and these can be

modified to represent more than just one

ten hour day

uh currently we have a person that just

responds to this in simulation

a fairly complex curtail and shift

person in the future i know this is

important

uh we will run this an experiment and so

we'll get actual data and we plan to

train generator models

so that so that this um this agent is

simulating something a bit more

realistic

finally our reward basically um

optimizes on the total price throughout

the day but we include a regularizer

so that the agent doesn't just reduce

energy

across the board but specifically during

uh

during certain hours

uh so so this pipeline is focused in an

office environment

but really you can think of it generally

as a system where an agent is trained to

incentivize

individual energy use in any context so

please

if this interests you at all think about

using it for whatever thing you're

you're thinking of doing

try it here it's at this github

conveniently packaged within a docker so

very easily

to install and again shameless copy from

the navy group

please look for an office learn

competition sometime in the pretty

far future now part two the agent itself

we have demonstrated that a vanilla soft

factory architecture

can run pretty well on this it trains

well

through repetition um and it converges

to a value

however it typically trains within 12

000 iterations which is

30 years um and so we need to improve on

that

so um the main thing that we that we

suggest to

improve on is planning so we have

constructed a planning model that mimics

the office

um the controller the the the

the planning model learns from from uh

data that's generated

becomes better and then the controller

trains at that overnight again and again

as many times as it wants

um to do better if anyone knows of dyna

suggested by

sutton in the 90s this is similar to

that

and in general this seems to perform

pretty well

across the price signal environments

that i mentioned earlier

we see that the planning model beats the

vanilla model in

all cases is probably the most

pronounced in the simplest case

so in time of use uh pricing over one

day

the vanilla models are on the bottom in

orange and the

other models are on top and they

converge a lot faster

so this is promising they converge

within a thousand times

which is um great that's like three

years

um however in the future

uh some things we're thinking of doing

uh right now we're thinking of reducing

the action space

so instead of outputting a 10

dimensional vector of prices

we're going to output uh the weights to

cosine series that represent these

prices

we're dealing with a time series so why

not exploit the structure of a time

series to make things better

if anyone has thoughts on this or has

tried this i'd really love to hear about

that so please let me know

uh second like i mentioned we're

conducting this in an actual experiment

uh we haven't we have an office in

singapore um that has around 50

office capacity due to covid we're

supposed to do this in july but you know

covered and we expect this experiment to

take three to four months and then we'll

have a lot of data

on how individuals in an office respond

to demand response

which as far as we can tell does not

exist anywhere

and we really have tried to find it um

generative modeling of office workers

using this data we hope to

train a series of generative flow models

so that we have an

agent that can generate their their

energy demand profile throughout the day

in a realistic manner and finally

um we'd like to uh try several different

forms of multi-agent reinforcement

learning

this is notorious for being very

data-heavy so we're not really sure if

this is going to be a success

but so far we want to try to meet

multi-agents

with centralized communication and mean

field

rl potentially with bing ching um any

other agents you're interested in trying

please email me at lucas underscore

sanger

um oh okay you have one minute

cool well i just finished so uh

does anyone have any questions let me

check on the chats

okay um i have a question about the

figure that you showed to have a

comparison between the vanilla ones and

the ones we have planning so we can see

obviously that planning is

um helping the learning process in terms

of competition cost and how

slow are they are they different i mean

i know they have the same

almost number of iterations but on and

you can see the convergence rate for the

planning ones is improving but how about

the computational cost

um that's a really good question step

for step

they're equivalent um

uh especially because in our simulated

office workers you're basically querying

a function

which is very similar to what you're

doing in the planning model you're

screwing a different function

um however in practice because you'd be

running

um you'd be running the planning times

many times during a single day

and running the vanilla one once during

a single day you'd encourage some

computational costs however i don't

expect this to be

limiting uh just because our step size a

day

is so long that you really do a lot

thank you

we have time for one more question if

anyone has any question

yeah yeah so yeah i wanted to ask so

would there be an energy bill for the

whole office

that they paid to the utility and then

each person will have an individualized

energy bill that they will pay to the

office

is that correct um basically

yeah and that's a really good point um

because under our current intuition of

things

one of the reasons why um individual

energy consumption in offices is a

problem

is that it's um you're contributing

towards a common cost

that you don't bear directly so

right right so so so in our our

framework specifically

we we run social games where people

compete against each other for

for prices and so in that way we

individualize

the bills uh so then so we individualize

uh

costs so does can the office make a

profit

like increase the prices that is

charging the

occupants and make a profit out of it

yeah there are several ways for the

office to benefit financially

um it will save energy we have

demonstrated this in previous

experiments

and so the office can save money

directly there

it can pass some of that money and that

savings

onto the workers in terms of incentives

um for doing this

um uh and if and i guess if we were

implementing what you're suggesting

then and the office doesn't have to

worry about its energy costs

collectively at all

then i guess it would save plenty of

money there

so what prevents the office of charging

five times

the price of electricity to the to the

occupants

for example um

there are some issues there and how to

manage the cost of common resources like

common lights

um uh

but i suppose not in theory nothing

really nothing prevents the office from

jacking up its prices to to the

consumers

no thank you lucas great presentation

um we're gonna move to the next

presentation

and we're gonna clap for you okay

so the next presentation is um by

doctor shangri-zhang he is a postdoc

researcher from the computational

science center

at the national renewable energy

laboratory

he received his ph.d degree in

electrical engineering from virginia

tech

currently he is interested in solving

energy system related control problems

with deep reinforcement learning and

other stochastic optimal control

technique on high performance computing

the floor is yours

thank you chef uh can you guys hear me

now yes

okay so can you guys see my screen not

yet

oh okay looks like it's a permission

issue again

um wow um

sorry about that uh looks like it

required me to

tweet the zoom and join back later so uh

chad would you mind

to can we switch to the next presenter

um okay i'll come back next uh next

round

thanks for this and sorry about this no

that's fine

okay um

oh one second um so i think i can shout

it now

um can you go ahead see this

yes okay do you see my slides here

not the slide i see your security i'm

probably sitting

okay you're on the right track then

so let me see i'll share the new screen

so i set this one and share

yeah okay yeah i can't even see it okay

great uh sorry about this

so i'll be quick um strictly seven

minutes so um

so hey hi everyone my name is xiang yu i

come from the computational science

center

and today i present paper on behalf of

our team here

and our title is transferable rl for

smart homes

so our motivation is that we would like

to see in the

real-time price dr market what is the

best way to implement a smart home

optimal control

in an affordable manner so affordable

means

the answer to this question will be

beneficial to

both the grid and the homeowner so the

state-of-the-art technology

nowadays are mostly based on the npc and

which can successfully achieve the

optimal control however the intrinsic

features for optimization-based methods

oftentimes prevent them from being cost

effective in the real-time

implementation

so the task for this this paper is we

would like to

investigate the feasibility of using rl

to develop smart home controller

and explore how to train them at scale

using transfer learning

so we envisioned uh az and cloud

integrated framework just look like this

on the edge side we have the home and in

the home we have iot devices collecting

the building operating data

plus some occupants preference data

and when the data is prepared they will

be uploaded to the cloud

where we will build a simulator and

using this simulator an rf control

policy will be trained and it will be

downloaded

back to the edge side for control so

this is the loop for one building

what can we do if we have thousands of

buildings thousands of homes who won't

have this service

the good news is that we can leverage

the similarity

among residential buildings because

homes they're similar from many

different perspectives here

so our idea is that we can leverage the

knowledge

from an existing controller of a similar

home to accelerate the training process

for a new controller

oops

so um basically we used a transfer

learning

our hypothesis is that for similar homes

uh their corresponding optimal

controller should also be similar

given that we already have a home h0 and

we already have a

existing rl controller parameterized by

this

and given a new home which are similar

to h0

what we will do is we will one start the

parameter

for hi using the h0 parameter

and then we will continue to use rl to

learn this policy for example here we

show the policy gradient method

and in contrast if we use a prl without

the knowledge transferring we will

randomize the parameter

in this new controller

on the right hand side is illustration

for the qrio approach

and the transfer learning approach so if

we use a qrl uh

approach we will need to train a

controller individually for each of the

homes

and if we use transfer learning we only

need to learn the first one

and then transfer the knowledge to all

other similar homes

and by doing this we hope we can save

some combinational resources

so to verify this idea in the case study

we

developed an optimal appliances

scheduling problem considering the

uncertainty from the rtp

and the goal is to minimize the cost

from occupants discomfort

as well as the energy consumption so we

consider four typical home appliances

and each of them have a specific

discomfort function

and operating constraints so we

we also use a real-world rtp data and we

formulate this optimal control problem

into a milp problem

uh because all the appliances here we

assume they are on off control

and objective is to minimize this thing

and the first term is the discomfort and

second term is the energy consumption

so to train our rl controller to solve

this problem we first formulate it into

mdp

with a reward structure which is

equivalent

to the objective function shown in the

previous slides

and uh with the formulated mbt ntp we

build ultimate gene environment and

then use a proximal policy optimization

to train it on the unreal hpc system

so the rtp data that we use for training

is from july

and we use first 10 days in august to

test

the performance and here are the

comparison between mpc and rl

algorithms and numerically we can see

that rl it can achieve a pretty close

average cost compared with the mpc

controller

that means rl can have the potential to

successfully handle this smart home

control

problem so this is for one home

and so we have what about

multiple homes so assuming that home is

a home

zero and we already have uh aria

controller for it

so here we build some new homes

and which they are similar to home zero

uh like

home one one and home one two they're

different from home zero in the

user preference and home two one and one

two two they're

different from the appliances parameters

for example

and the home three one and home three

two uh we basically add some

or all the differences mentioned above

here

and for all these six cases uh we

compiled

the tl accelerated rl and the

reinforcement

qrl uh in these six cases and these are

the learning curves showing

up here and we can see that by using our

uh tl accelerated rl um in many cases we

can save

a lot of like computational resources uh

except

okay except these cases so um

based on this we have two notable

observations

the first one will be tl plus rl is

better than purl

if we can give it a good starting point

and the second is that tl advantage does

not change smoothly with the difference

between the source and target buildings

and another way we can put it like the

home similarity

indicating the tl acceleration is not

proportional to the equilibrium distance

in the home parameter space

which is a common idea of how we are

going to measure the similarity of the

home

and that just doesn't work out uh so our

next step will be like what is the best

way to

cluster the home so that we can based on

their similarity we can do the transfer

learning

and lastly i want to point out that we

did use the transfer learning to

transfer

knowledge between our controllers in

this case study

and in another uh project we did a

similar idea but we transferred

knowledge between

two different rl algorithms we propose a

global local policy

search basically in this case we

transfer the knowledge between

an esri algorithm and a ppo algorithm to

search a control policy

so that concludes my presentation

happy to take any questions thank you

very much thank you zhang

great presentation um so you have a

couple of questions on the chat

the first one is how how do you identify

the similarity between the homes any

physical metrics that you are using

for this so in this

um preliminary uh study we we basically

um create manually created all these

homes which are similar to the home zero

so i say this is just a premier

preliminary experiment so that we can

see

if two homes they are similar we can do

this however in this paper we

didn't really come up with a good way

to cluster homes to identify their

similarity but we do have a finding that

uh as i mentioned here the including

distance in home parameter space

uh that is just not good for such kind

of similarity

uh measuring okay

you have one more question but since

we're running out of time if you don't

mind answering it on the chat box for

zoom that would be great

thank you yeah we'll do thank you very

much thank you

so um the third presentation is

um the presenter is anand krishnan who

is a scientific engineer

at berkeley national laboratory and

works in building technology department

his research focuses on using network

connected devices in building for

monitoring and

advanced control to improve building

operations

and he's also involved in several

projects that study

grid building interactions for demand

response resiliency and load flexibility

hi everyone um let me share my screen

cool can you see my screen yes

okay cool um

let me use that hello everyone as you

throughout the workshop today

resource spin learning has great

potential to advance spelling operations

and controls and help us transition

towards a cleaner future

we've seen reinforcement learning

controllers

can be used to minimize costs optimize

comfort and support the power utility

however we do have to talk about the

elephant in the room

the challenges researchers face when

they attempt to deploy

deep reinforcement based controllers in

actual buildings

are the significant challenges you know

that have occurred during this process

such as uh uh integration with

accessibility auto image system

you know dealing with faulty sensors our

paper focuses on the difficulties that

arise due to the assumptions made during

the training and development of a

reinforcement learning controller and we

attempt to quantify the impact of these

assumptions

my name is anne prakash and i'm a

scientific engineer this work has been

done by our team at lawrence berkeley

national

laboratory we developed and deployed

our trl controller at the flexlab

testbed at buckley lab

with the objective to minimize energy

costs flex lab emulates a single zone

commercial building with onsite solar

generation and electrochemical storage

it also contains emulated humans

schedule based lighting and plug loads

and the indoor environment is

conditioned by an air handling unit

as trading a drl controller directly on

real building

can cause damage to the equipment and

discomfort to the occupants we used off

policy learning

to train our agent we trained the agent

on an opening a gym environment that use

energy plus modelica and functional

mock-up units

to model the building envelope the

attack systems and the you know

distribute energy resources

additionally we also use four years of

historical weather and electricity

tariff data for training as well

we chose the deep deterministic policy

gradient or ddpg as a drl algorithm

as you can see there are several

assumptions made in this phase that get

embedded into a training agent

when the strained agent is deployed in

the actual building these assumptions

can impact the

control actions it generates and the

following slides we look at them in more

detail

random seeds are used to initialize the

agent during training

and two controllers using the same seeds

would produce the same actions

conversely controllers that controllers

using different seeds would produce

different

actions the first assumption that we

made was with sufficient training the

variance in the actions generated by the

controllers

trained on different seeds would be

negligible and hence the process of

choosing the seed would be hardly

significant

however this was immediately proved

false out of eight similarly configured

controllers that were using the same

hyper parameters but initialized with

different seeds

only four of them converged the

distribution of the rewards of these

four controllers are plotted on the left

next we choose the we choose two

controllers with the most

similar reward distributions that is

kind of controllers initialized with

seed one and c2 and compare the actions

they generated for our building

this can be seen on the figure on the

right where the significant difference

in temperature set point generated

for the supplier is quite evident

moving on to the second assumption we

assume that the building model that was

calibrated using standard practices

would suffice

to trade a deep reinforcement learning

agent to produce optimal actions

here we have two controllers that were

trained on two different building energy

models

where the only difference between the

models being the fan being the power

consumed by the supplier fan

the figure on the left shows the fan

power consumption from two energy models

simulated with the same sequence of

operations

the poorly calibrated model assumes that

the supply fan consumes

constant power irrespective of the

command it has to be noted that the

supply of fan power is only eight point

five percent of

the total building load in spite of this

small contribution

when the corresponding controllers were

used to generate actions for the supply

of flow rate in a building as you can

see on the figure on the right

the controller that was trained with the

uncalibrated fan does not actually the

supply fan at all irrespective of the

state of the building

this is an illustration of how inc how

the inconsistencies between a simulated

building and a real building can impact

the performance of an algorithm

the final assumption that we studied

also arises from the inconsistencies

between the model of a building and the

actual building

during training using the building

energy model it is assumed that the

building equipment can condition their

environment to meet the requested set

point

however in reality this assumption

hardly ever holds true

due to the physical constraints or the

lower level control sequence of the

equipment

it may not even be able to reach the set

point generated by the diagonal

controller

the figure on the left the supply

temperature set point generated by drl

controller and compares with the actual

temperature

between 3 p.m and 6 p.m there's almost a

difference of 5 degree celsius

we observed similar behavior in our

battery as well and to study

how this lack of awareness of the

physical constraints can impact a dll

controller we compare the set points

generated by the same controller in two

different environments

one with the real battery and one using

an emulated battery

the difference being that the emulated

battery is able to reach the battery set

point immediately

it can be seen that the emulated battery

has almost a 10 higher state of charge

during the periods of 3 pm to 9 pm

and such incorrect state of the system

can lead to sub-optimal actions being

generated

this table summarizes the differences in

the actions generated for the

ahu and the battery and when the

assumptions we made in training no

longer hold true

the largest impact across all three set

points arose from using a controller

that was trained on an inaccurate

building energy model

preliminary evaluation also revealed

that a controller that was deployed in a

real building

can incur more than twice the expected

energy cost

obtained from simulation it is also

important to point out that in our

evaluations we isolated these

assumptions and studied them separately

this would not be a realistic scenario

to conclude it is evident that the

assumptions that were made during the

development and training of a trl

controller

can have significant impact on its

performance in a real building

for next steps we're looking at studying

the impact of these assumptions when we

use different drl algorithms instead of

gdpg

this might help alleviate some of the

some of the issues we

encounter when we use the random seeds

we are also trying to identify standard

metrics to

quantify the impact or quantify this

impact irrespective of the problem

objective

and the algorithm used and also we are

trying to develop better training

environments to incorporate the negative

impact of these assumptions

um i guess that's it and thank you i'll

take any questions now

thank you for the great presentation let

me look at the chat box

okay i still don't see anything um

um so i have a question for you so

basically this is um

very common whenever we want to deploy

reinforcement learning just to

to validate and pre-train the model in

simulation environment and then deploy

yeah so based on your experience and the

data and the information that you just

provided to us

then how do you see us that what is the

best simulation test but that we can

just pre-train our model before

deploying it

in the field i think any model that you

train

is going to be inconsistent with

real-world systems so

the um some of the steps that we took

was after we had a model we

still did a lot more calibration when we

identified we had issues

so the you know we when when i said our

model was calibrated your standard

practices we

we initially had a normally normalized

mean bias error of less than 10 percent

with the action system

and in spite of this is we saw all these

issues pop up

so the only way is like characterize the

systems better identify what control

sequences are used in the system

and try to incorporate the control

sequences in the modeling also

so that might help you know identifying

or better that might help deep

reinvestment learning understand

understand the performance and the

behavior of the equipment better

so better modeling basically and

calibrating

thank you does anyone have we have time

for another question if someone has a

question

okay i think we can go to the next

presentation

thank you um

so binking will be the next presenters

um she

is a psja student from carnegie mellon

university

her research focuses on enabling real

world reinforcement learning for

building control and she won the best

paper award at building since 2019 for

her prior work

for on this topic so congratulations for

the airport

you can start your presentation

myself thank you helia for the

introduction so the objective of this

work

is to introduce wait

wait wait a second let me let me make

sure that i'm using the correct

deck of slides okay

okay um yeah the objective of this work

is to introduce off policy evaluation

as an essential prerequisite towards

real real-world reinforcement learning

for building control

this work is motivated by a problem we

encountered in our prior work

previously we proposed neural which

incorporates domain knowledge on

planning and system dynamics

furthermore we demonstrated that url

agents

pre-trained with imitation learning

could match the performance of existing

controllers

thus enabling real-world deployment of

such

agents using only historical data

however it's less clear to us how

imitation loss during pre-training

translate to actual control performance

during real-world deployment

to elaborate on this point let me show

you a specific

example here i'm showing the imitation

loss

during pre-training while the imitation

loss decreases over epoch

does that imply the energy and comfort

cost

decreases as well well the

answer unfortunately is not really

while the comfort cost decreases with

the loss

the energy g decreases first

and then increases this reaffirms our

initial point

that imitation loss does not directly

translate to control performance

this motivates us to incorporate off

policy evaluation

as an addition to our framework of

policy evaluation

is the problem of estimating a policy's

performance

without running it on the actual system

using historical data from the existing

controller

of policy evaluation is a classical

method

and thus there is a lot of existing

methods

the question we are trying to answer in

this work is that

which of these methods are appropriate

given the characteristic

of building operational data

the challenge here is that most

buildings today

are still operated by rule-based control

which

are deterministic policies as a result

these policies only explore a very small

portion of the state action space as

is exemplified here using historical

data from a real-world system

after pre-training a policy one would be

interested in

evaluating how well would the policy

perform

intuitively this question is challenging

because there is

no observation for these state action

pairs

in our historical data set in fact

this characteristic violates the minimal

requirement

of many ope methods for

that we refer you to our paper for more

details

given this observation we need

a method that can extrapolate to unseen

state action space

thus we adopt the approximate model

method

which use a data-driven model learned

from historical data

as a proxy for the actual environment

with the help of the model one can

extrapolate to unseen state

action space

to validate our proposed approach we use

an energyplus model as a simulation

testbed

this environment this energyplus model

is based on a actual environment on

carnegie mellon campus

and it's a mod in this system

we control the supply water temperature

to a water-based radiant heating system

to maintain the zone temperature we use

the simulation test bed

to generate historical data and evaluate

the ground choose performance of target

policy

to demonstrate the generalizability of

our proposed

approach we pre-trained 10 target

policies with imitation learning

firstly we demonstrate we illustrate the

use of our method

on one particular target policy

here we are showing a comparison of the

state

and action trajectory of the same target

policy

in the energy plus model as shown here

in the blue line

and then the in the approximate

model as shown in the green line as you

can see here

the blue line and the green line are

qualitatively similar

this means that an approximate model may

serve as a reasonable

proxy of the actual environment for

performance

evaluation at the same time you can see

that there

is some discrepancy in the zone

temperature between the actual energy

plus model

versus the approximate model which will

explain later why our evaluation of

comfort

performance is less accurate

furthermore we use bootstrapping to

correct for bias

and quantify uncertainty the shaded

green area shows a histogram

shows the histogram of bootstrap

estimates and the blue line is the

ground shoes performance

as you can see for the energy cost the

ground shoes false was

the force in within the empirical

confidence interval

from the bootstrap samples while the

comfort cost

does not the reason is that

bootstrapping accounted for parameter

uncertainty

in model estimation but there are two

additional sources of

sources of biases that it cannot

account for we also refer you to our

paper for

detailed discussion finally we show

a summary of the

first summary over the 10 target

policies that we consider

on the x-axis we have the ground choose

performance

and on the y-axis we have the estimate

from the approximate

model estimate and the confidence here

interval shown here is estimated using

the boost drop samples

on average the approximate model method

estimated the energy and comfort cost

within a 1.84

and 14.1 percent error respectively

to summarize this work the main

objective here

is to introduce off-policy evaluation to

the building control

community as an essential prerequisite

for real-world reinforcement learning

an important intuition here is that the

characteristic

of building operational data presents

challenges for applying many existing

methods

to building control

we the approximate model method

shows promising results in this

preliminary work

and allows one to tap into existing

knowledge

on building models to make informed

decision on model forms

and modeling methods however it by no

means

precludes the possibility of other more

sophisticated methods

being more suitable for building control

finally while the approximate model

allows us to extrapolate to unseen state

action space through its assumed model

form

it also introduces a bias which is not

corrected for

using bootstrapping thus further

research is required to address this

challenge

this is the end of my talk um thank you

thank you great presentation

let me see if the chat to see if we have

any questions

i have a quick question yes um

uh so your grey box kind of answered it

for me

but i'm wondering in general are the

errors between your model and the

plus models right like like is there

like predictable bias there or is it

random

um is the question as to

um if i interpret the question correctly

um of course that if you have a more

accurate

approximate model the estimate is going

to be more

accurate so there is so the method

is um the accuracy of the method depends

on the quality of the model

if that answers your question

is sorry i'm so sorry i i want to leave

time for someone else to answer a

question

but but my question was like is like is

it

is there is there bias in this error

like could you imagine like a secondary

model

that um that corrects for this bias

really approximates between the output

of your model and

what you're trying to predict i see i

see so this is what i'm trying to

um kind of convey in my last

bulletin point on this slide that is

that

we basically by assuming

a certain model form uh we

enable extrapolation but we are also

introducing

a bias through the functional

approximation

and then like this bias

is basically very difficult to correct

using data alone which is demonstrated

by like

the bootstrap estimate didn't manage to

correct for that because that's like a

structural we are making an assumption

for the model form

and we don't exactly know how that will

impact our estimate using a data-driven

approach

yeah thank you for the good is it done

with the

question um

i have of course i've followed questions

but but no please like like the floor is

open for

for someone else for sure i'll ask them

privately

okay yeah thank you for your

presentation so yeah i wanted to ask you

if you think

that the previous approach that you were

using with imitation learning

has any advantages with respect to this

approach using of policy and learning

from historical data

yeah so um

regardless so so

i saw that in your review i think

regardless of what methods whether we're

using

imitation learning or off policy eval

of policy reinforcement learning like

they all face the same

issue that you're trained you have a

training loss

it's unclear how that training loss is

related to

control performance so we basically it

so off policy evaluation is analogous to

you having a validation data set during

say supervised learning it is the

intention is to um

for validation it is not for

like training and with regard to

um whether i think uh imitation learning

is better or of policy reinforcement

learning

is better i think this is a very nuanced

question because

then the question is what am i

comparing like the state of arts

imitation

learning method versus the state of yeah

i was asking not if one is better than

the other

that for i mean it seems that for what

you are saying the of policy method is

better

but if the imitation learning offers any

advantage

that the other method learning from

historical data doesn't offer

i i'm not sure i got the

question

yeah i'm saying that if learning from

some sort of model or other control

using imitation learning has any

advantage

over just taking historical data from a

controller

and using enough policy algorithm to

learn from it

yeah um so the first point i'm trying to

like

say is that um the work this work this

particular work is about off policy

evaluation that is like a separate

separate step from of policy

reinforcement learning

and um the second question with respect

to

advantage just versus disadvantages

what i was trying to convey

is that i don't think there is a

straightforward

answer to which class of method um

is better i think uh it needs to be

answered on a

algorithm by algorithm basis because

both are areas that are moving very very

fast i can't behave compare behavior

cloning to the state of art

of policy reinforcement learning that is

not a fair

that is not a fair comparison yeah

that's

so um i guess the short answer is that i

don't have a

good answer to that question

okay yeah that's fine thanks thank you

binking for the

great presentation um we're gonna go

back to the panel discussion so if you

have any other questions you can use

through the either slide

or zoom chat um so i have one quick

question to just

start the floor so since the title of

the session is keeping it real so based

on your experience and what you have

seen um what is the main challenge you

see to deploy reinforcement

learning algorithm for optimizing and

building specifically because

we are talking about building here um at

the scale so what challenges you are

seeing

and um if you have any experience doing

that we would be happy to hear about it

um i could maybe take a stab at that um

i mean i think these are challenges it's

not just limited to deep reinforcement

learning

but some of the main things are uh

the especially in our case we used

enough policy learning and we had a

model and we didn't use data like

how do you know the model uh is good

enough first of all

and then how do you know it would still

encapsulate all the different state

action space

that is necessary to train it so i think

an approach to solve this was using a

lot of data

and hopefully it and it covers

everything

but it still may not be so i think there

needs to be some sort of metric

to determine yeah this is good enough

and

this might help us at least

set a starting point i think uh

benching's previous paper i remember

engine talked about training it on the

real system

to get some data and then start control

so that might be helpful

too

i might have simplified it too much but

yeah well

but yeah i think anan's point is very

relevant it's kind of what i'm

trying to address like trying to at

least like

trigger people to think about this

problem it's that

regardless of what algorithm we are

using we can train

way we're training it we can have a

policy

but then how do we know it's good enough

how do we know it's good enough it's

actually

unclear because we don't know if we are

not sure if the simulation model is good

enough

or if we are looking at some kind of

training laws we don't know if based on

the

based on the training laws we don't know

how that translates to control

performance

so whatever paradigm we are training

pre-training the agent

and there is a big over big

question as in how do you just

at what point are you confident

about a policy that you have untrained

and then the next step is how do you

convey

the expected performance to the building

control

um building like the facility management

so that they don't just

take your word on faith you can actually

give them something that is more

tangible and like more so you can make a

statistical guarantee to them

so that like everyone is has to build

confidence about these algorithms

i have a quick general question for for

for everyone i guess

um and i'm sure that you've heard of

recent like really exciting advances in

the field of

view shot learning um and of course like

that domain

is a little different it's more like

around more straightforward supervised

examples um you think that techniques

like that will eventually make their way

over into like tools we use like

reinforcement learning

nets that can actually minimize our data

requirements

lucas uh could you repeat the new topic

that is coming up

the name for that um the name that i'm

aware of is few shot learning or

one-shot learning

um open ai uh

like open ai's gpt-3 i think made it

made a lot of splash because as it

increased the number of parameters in

its net

it actually seemed to decrease the

number of training examples needed

um which is obviously like a constant

critique of reinforcement learning

yeah i i think that corresponds to

like the point that we discussed in like

session two that's

by incorporating some kind of domain

knowledge

whether it be like the incorporation of

a model

or prior experience of um

um what type of parameter works well

these are all things like there is we

have like a

large knowledge base of

knowledge that can be incorporated to

increase the sample efficiency

that's my take and i think there are

a lot of basically there are many ways

to

approach the problem maybe like as you

said field shot learning

matter learning or transfer learning

these are all things that can be

incorporated

okay does anyone other of the panelists

have any other ideas

thank you everyone for your input by the

way um

i i think donghun has a question in the

chat like i'm happy to read it aloud

if you'd like oh i can't read it thank

you

oh my question is that

if a reinforcement learning needs to

build a simulation model for training

why don't we just build an npc model

um so

so my answer is that uh reinforcement

learning

don't need to build a simulation

model for training that's um kind of

what i was trying to address in my prior

work

that we can by basically

incorporating domain knowledge and by

pre-training the agent with historical

data

we don't need a high fidelity simulation

to pre-train the agent

and of course creating this simulation

environment

is extremely resource intensive so i

think it's an

important problem to understand how do

we

train reinforcement learning agents

without the need for those models

so i want to add on that um another

thing is that

we can use some data driven model which

can create for example

if you have a if you leverage for

example the random forest

this kind of machine learning model they

can have very good

uh forecasting property

to learn the building thermal model

however a lot of

machine learning models like this

they're not differentiable

so in that case you can leverage this

kind of

beta-driven model in reinforcement

learning but not in the

optimization based approaches so that is

just

uh one idea of why um

rl can be a advantage here

another advantage i think is that uh mpc

methods are finite horizon models right

theoretically reinforcement learning is

learning over

an infinite horizon and it gets better

and better if you have uh

have good reward functions so that that

could be a difference too right so

like sorry i'm i'm not a panelist but

can i jump in uh so

in terms of that question right i mean

weather data

anyway in a mathematical framework for

the reimbursement learning we need to

know the all the

state trajectory for infinite horizon

but it doesn't really make sense right

it's not just a matter of the prediction

horizon because we don't know

what's going to happen in a visual like

100 years later

in terms of weather and solar right

yeah that that's a good point right but

uh

then that that's why for example we do

relearning

right we know that we are not going to

be able to predict all patterns

in the future but then if you can detect

a change in pattern you can then try to

relearn

but then you have to be careful how you

relearn you don't want to do

catastrophic forgetting because you'd

have to forget what you

what you learned before because those

patterns will re-occur

again right npc does that so

uh because the you know the you see it's

basically receiving her item control

if something is not captured within the

prediction horizon

and mpc moves to the next step and with

the updated

information compensate the disturbances

mismatch

and keep this respect those kind of

approach

for the motor predictor controls uh so

anyway i i just brought up those

questions because i'm real

i'm heavily involved in the mpc uh

algorithm

and just my suggestion maybe next time i

don't know

uh jorton uh maybe it's a good to

involve

npc uh you know the specialist or

expertise and have like a integrated

mpc and reinforcement learning workshop

something like that to to uh

identify which which place we should go

for

the building operation just just a small

comment to your work i have seen mpc

work where they actually

relearn the model in a data driven model

from activity

um i also have like very limited

knowledge comparing

mpc and greenfield like not in the same

building but we've deployed mpc based

controls in

a different environment testbed as well

and we've seen

that under like extraordinary or

not seen before circumstances uh for

example the fires that happen in

california like mpc does not find

does not is not able to find an optimal

solution and just fails to converge

and this like produces really unoptimal

set points for the building

and i with my limited knowledge in

reinforcement learning i i've

what we've seen here is that it has the

ability to

better handle such unforeseen

circumstances

so that could be an advantage like for

example

the mpc building that we had we had

customers come call us and complain that

people are melting and so are chocolates

so probably deep reinforced learning

might have a better you know again this

is just speculation

yeah yeah so so that's my question

actually at this moment i don't think

we still have every kind of a problem

that works for every type of building so

i wanted to

kind of uh identify the

what is the best solution for which type

of building

under what scenario i don't believe like

a reimbursement montana can couple

every building and every problem that's

why i'd like to

kind of include some of the folks from

the motor predictive control because

we haven't studied like more than two

decades

in the npc and a lot of issues coming in

in this workshop

has the same issue i can hear like

you know the what is the the the best

motor structure or hot bubbly

space model and like you know the what

is the comfort

uh index how can we uh you know the

come up with the best like a baseline

structure

anyway that's been discussed for decades

in an mpc

uh the field so i'd like to maybe

involve some other expertise in the mbc

and have a

discussion together not just yeah

yeah right thank you thank you very much

lucas

one second lucas um because i i want to

be mindful of your time and

we're 10 minutes past uh the scheduled

time and i know some of us

some of you guys are in europe and it

gets later and later

but we're exactly where this

conversation would be going

in an awesome direction if we were to be

able to continue it in a room

um and and before we do that i do want

to

say you know thank you everyone um we

will we had a great conversation thank

you for the presenters thank you for the

organizers thank you for the session

chairs

uh those who put together the program um

what i would like to do

there's two things one is i would like

to solicit some kind of feedback

whatever you want put it in the chat

send me an email put it in the

in the slack let me let us know should

we do this again

if we do this again next year should we

invite npc

people or not as someone suggested just

as a as a plug

um i mean it should be uh and then make

a bigger clash of the algorithm session

or what else you would like to see and

and you know who would like to

contribute or who would like to be part

of this

bigger picture and as for closing out

the um the the one thing that built this

organizers have put together nicely is

this interface called getter town for

those of you who checked it out let me

share quickly my screen

and show you so

you you will get the link in the chat

box in a one second

right now you can walk over there

um you you will you will need a chrome

browser

put in your name um join the gathering

as it says

oops not this one this one um

and then you will be in the conference

room

outfitting and you can navigate out of

this in your 8-bit world

go ahead

comment okay so never get out of this

because nobody wants to be in the stuffy

conference room of course all day long

so we go to the outdoors

let's move to the beach garden you see

there's people there and you get the

videos tuning in

so move to the beach garden and enjoy

the view and the breeze

uh once it comes in here we are

yokama beach um and

hang out here to be a garden um put in

your name

put something that identifies you as

arlem so we can see you

approach each other and you'll see your

videos um

showing up try to find the presenters

presenters maybe identify yourself if

you're joining

so that people can find you and then you

know i'll hang out there

i'll be back in a second and then let's

see where this conversation is going

and then everybody else um you know the

papers are online

get in touch if you have feedback use

the use the slack channel it will be

working

until the end of the conference at least

and then you know get in touch keep the

community growing and let's try to meet

again next year at least

latest and then you know see where this

is going

thank you very much this ends the

session and the conference and the

workshop not the conference of the

workshop

oh one last thing i'm sorry before i'll

really let you go

since it has been mentioned in the slack

channel there is one session in both

this that also deals with rl and there

are several posters that build this that

also deal with arrel

who were not presented here and so

there's more to see and more to talk to

and with that thank you

good night for those in europe good

morning indonesia

and it's happy hour in the u.s so

let's do that bye-bye thank you

thank you everyone

[Music]

thank you

you

